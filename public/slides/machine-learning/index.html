<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>CFSS: Computing for the Social Sciences</title><meta name=description content='<!DOCTYPE html> An introduction to machine learning class: center, middle, inverse, title-slide .title[ # An introduction to machine learning ] .author[ ### INFO 5940 Cornell University ] --- class: inverse, middle # What is machine learning? --- class: middle <img src="../../../../../../../../img/amazon-recommendations.png" width="80%" style="display: block; margin: auto;" /> --- class: middle <img src="https://miro.medium.com/max/1400/1*j5aWfH9t1_EZPJC92CJ7oQ.png" width="80%" style="display: block; margin: auto;" /> .footnote[https://medium.com/tmobile-tech/small-data-big-value-f783ceca4fdb] --- class: middle <img src="https://techcrunch.com/wp-content/uploads/2017/12/facebook-facial-recognition-photo-review.png?w=730&amp;crop=1" width="80%" style="display: block; margin: auto;" /> --- class: middle, inverse # What is machine learning? --- class: middle, center <img src="https://imgs.xkcd.com/comics/machine_learning.png" width="40%" style="display: block; margin: auto;" /> --- class: middle <img src="images/intro.002.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: middle <img src="images/intro.003.jpeg" width="80%" style="display: block; margin: auto;" /> --- ## Types of machine learning - Supervised - Unsupervised --- <img src="images/all-of-ml.jpg" width="80%" style="display: block; margin: auto;" /> .footnote[Credit: <https://vas3k.com/blog/machine_learning/>] --- ## Examples of supervised learning - Ad click prediction - Supreme Court decision making - Police misconduct prediction --- ## Two modes -- .pull-left[ ### Classification Is your hospital in a state of crisis care? ] -- .pull-left[ ### Regression What percentage of hospital beds are occupied? ] --- ## Two cultures .pull-left[ ### Statistics - model first - inference emphasis ] .pull-right[ ### Machine Learning - data first - prediction emphasis ] --- name: train-love background-image: url(images/train.jpg) background-size: contain background-color: #f6f6f6 --- template: train-love class: center, top # Statistics --- template: train-love class: bottom > *"Statisticians, like artists, have the bad habit of falling in love with their models."* > > &amp;mdash; George Box --- ## Predictive modeling ```r library(tidymodels) ``` ``` ## â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.0.0 â”€â”€ ``` ``` ## âœ” broom 1.0.0 âœ” recipes 1.0.1 ## âœ” dials 1.0.0 âœ” rsample 1.1.0 ## âœ” dplyr 1.0.9 âœ” tibble 3.1.8 ## âœ” ggplot2 3.3.6 âœ” tidyr 1.2.0 ## âœ” infer 1.0.2 âœ” tune 1.0.0 ## âœ” modeldata 1.0.0 âœ” workflows 1.0.0 ## âœ” parsnip 1.0.0 âœ” workflowsets 1.0.0 ## âœ” purrr 0.3.4 âœ” yardstick 1.0.0 ``` ``` ## â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€ ## âœ– purrr::discard() masks scales::discard() ## âœ– dplyr::filter() masks stats::filter() ## âœ– dplyr::lag() masks stats::lag() ## âœ– recipes::step() masks stats::step() ## â€¢ Search for functions across packages at https://www.tidymodels.org/find/ ``` --- ## Bechdel Test <img src="https://fivethirtyeight.com/wp-content/uploads/2014/04/hickey-bechdel-11.png" width="60%" style="display: block; margin: auto;" /> .footnote[Source:[FiveThirtyEight](https://fivethirtyeight.com/features/the-dollar-and-cents-case-against-hollywoods-exclusion-of-women/)] --- ## Bechdel Test 1. It has to have at least two [named] women in it 1. Who talk to each other 1. About something besides a man -- ```r library(rcis) data("bechdel") glimpse(bechdel) ## Rows: 1,394 ## Columns: 10 ## $ year <dbl> 2013, 2013, 2013, 2013, 2013, 2013, â€¦ ## $ title <chr> "12 Years a Slave", "2 Guns", "42", â€¦ ## $ test <fct> Fail, Fail, Fail, Fail, Fail, Pass, â€¦ ## $ budget_2013 <dbl> 2.00, 6.10, 4.00, 22.50, 9.20, 1.20,â€¦ ## $ domgross_2013 <dbl> 5.310703, 7.561246, 9.502021, 3.8362â€¦ ## $ intgross_2013 <dbl> 15.860703, 13.249301, 9.502021, 14.5â€¦ ## $ rated <chr> "R", "R", "PG-13", "PG-13", "R", "R"â€¦ ## $ metascore <dbl> 97, 55, 62, 29, 28, 55, 48, 33, 90, â€¦ ## $ imdb_rating <dbl> 8.3, 6.8, 7.6, 6.6, 5.4, 7.8, 5.7, 5â€¦ ## $ genre <chr> "Biography", "Action", "Biography", â€¦ ``` --- ## Bechdel test data - N = 1394 - 1 categorical outcome: `test` - 9 predictors --- class: middle <img src="index_files/figure-html/unnamed-chunk-11-1.png" width="80%" style="display: block; margin: auto;" /> --- class: inverse, middle # What is the goal of machine learning? -- ## Build .white[models] that -- ## generate .white[accurate predictions] -- ## for .white[future, yet-to-be-seen data]. -- .footnote[Max Kuhn & Kjell Johnston, http://www.feat.engineering/] --- ## Machine learning We&#39;ll use this goal to drive learning of 3 core `tidymodels` packages: - `parsnip` - `yardstick` - `rsample` --- class: inverse, middle ## ğŸ”¨ Build models with `parsnip` --- class: middle, center, frame ## parsnip <iframe src="https://parsnip.tidymodels.org" width="100%" height="400px" data-external="1"></iframe> --- ## `glm()` ```r glm(test ~ metascore, family = binomial, data = bechdel) ## ## Call: glm(formula = test ~ metascore, family = binomial, data = bechdel) ## ## Coefficients: ## (Intercept) metascore ## 0.052274 -0.004563 ## ## Degrees of Freedom: 1393 Total (i.e. Null); 1392 Residual ## Null Deviance:	1916 ## Residual Deviance: 1914 AIC: 1918 ``` --- class: center <img src="images/predicting/predicting.001.jpeg" width="80%" style="display: block; margin: auto;" /> --- ## To specify a model with `parsnip` 1. Pick a model 1. Set the engine 1. Set the mode (if needed) --- ## To specify a model with `parsnip` ```r logistic_reg() %>% set_engine("glm") %>% set_mode("classification") ## Logistic Regression Model Specification (classification) ## ## Computational engine: glm ``` --- ## To specify a model with `parsnip` ```r decision_tree() %>% set_engine("C5.0") %>% set_mode("classification") ## Decision Tree Model Specification (classification) ## ## Computational engine: C5.0 ``` --- ## To specify a model with `parsnip` ```r nearest_neighbor() %>% set_engine("kknn") %>% set_mode("classification") ## K-Nearest Neighbor Model Specification (classification) ## ## Computational engine: kknn ``` --- ## 1\. Pick a model All available models are listed at <https://www.tidymodels.org/find/parsnip/> <iframe src="https://www.tidymodels.org/find/parsnip/" width="100%" height="400px" data-external="1"></iframe> --- ## `logistic_reg()` Specifies a model that uses logistic regression ```r logistic_reg(penalty = NULL, mixture = NULL) ``` --- ## `logistic_reg()` Specifies a model that uses logistic regression ```r logistic_reg( mode = "classification", # "default" mode, if exists penalty = NULL, # model hyper-parameter mixture = NULL # model hyper-parameter ) ``` --- ## `set_engine()` Adds an engine to power or implement the model. ```r logistic_reg() %>% set_engine(engine = "glm") ``` --- ## `set_mode()` Sets the class of problem the model will solve, which influences which output is collected. Not necessary if mode is set in Step 1. ```r logistic_reg() %>% set_mode(mode = "classification") ``` --- class: your-turn ## Your turn 1 Run the chunk in your .Rmd and look at the output. Then, copy/paste the code and edit to create: + a decision tree model for classification + that uses the `C5.0` engine. Save it as `tree_mod` and look at the object. What is different about the output? *Hint: you&#39;ll need https://www.tidymodels.org/find/parsnip/* <div class="countdown" id="timer_62f3ecba" style="right:0;bottom:0;" data-warnwhen="0"> <code class="countdown-time"><span class="countdown-digits minutes">03</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code> </div> --- ```r lr_mod ## Logistic Regression Model Specification (classification) ## ## Computational engine: glm tree_mod <- decision_tree() %>% set_engine(engine = "C5.0") %>% set_mode("classification") tree_mod ## Decision Tree Model Specification (classification) ## ## Computational engine: C5.0 ``` --- class: inverse, middle ## Now we&#39;ve built a model. -- ## But, how do we .white[use] a model? -- ## First - what does it mean to use a model? --- class: inverse, middle, center ![](https://media.giphy.com/media/fhAwk4DnqNgw8/giphy.gif) Statistical models learn from the data. Many learn model parameters, which *can* be useful as values for inference and interpretation. --- ## A fitted model .pull-left[ ```r lr_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel) %>% broom::tidy() ## # A tibble: 3 Ã— 5 ## term estimate std.error statistic p.value ## <chr> <dbl> <dbl> <dbl> <dbl> ## 1 (Intercept) 2.70 0.436 6.20 5.64e-10 ## 2 metascore 0.0202 0.00481 4.20 2.66e- 5 ## 3 imdb_rating -0.606 0.0889 -6.82 8.87e-12 ``` ] .pull-right[ <img src="index_files/figure-html/unnamed-chunk-27-1.png" width="80%" style="display: block; margin: auto;" /> ] --- ## "All models are wrong, but some are useful" <img src="index_files/figure-html/unnamed-chunk-29-1.png" width="80%" style="display: block; margin: auto;" /> --- ## "All models are wrong, but some are useful" <img src="index_files/figure-html/unnamed-chunk-31-1.png" width="80%" style="display: block; margin: auto;" /> --- ## Predict new data ```r bechdel_new <- tibble(metascore = c(40, 50, 60), imdb_rating = c(6, 6, 6), test = c("Fail", "Fail", "Pass")) %>% mutate(test = factor(test, levels = c("Fail", "Pass"))) bechdel_new ## # A tibble: 3 Ã— 3 ## metascore imdb_rating test ## <dbl> <dbl> <fct> ## 1 40 6 Fail ## 2 50 6 Fail ## 3 60 6 Pass ``` --- ## Predict old data ```r tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel) %>% predict(new_data = bechdel) %>% mutate(true_bechdel = bechdel$test) %>% accuracy(truth = true_bechdel, estimate = .pred_class) ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.580 ``` --- ## Predict new data .pull-left[ ### out with the old... ```r tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel) %>% predict(new_data = bechdel) %>% mutate(true_bechdel = bechdel$test) %>% accuracy(truth = true_bechdel, estimate = .pred_class) ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.580 ``` ] .pull-right[ ### in with the ğŸ†• ```r tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel) %>% * predict(new_data = bechdel_new) %>% * mutate(true_bechdel = bechdel_new$test) %>% accuracy(truth = true_bechdel, estimate = .pred_class) ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.333 ``` ] --- ## `fit()` Train a model by fitting a model. Returns a parsnip model fit. ```r fit(tree_mod, test ~ metascore + imdb_rating, data = bechdel) ``` --- ## `fit()` Train a model by fitting a model. Returns a parsnip model fit. ```r tree_mod %>% # parsnip model fit(test ~ metascore + imdb_rating, # a formula data = bechdel # dataframe ) ``` --- ## `fit()` Train a model by fitting a model. Returns a parsnip model fit. ```r tree_fit <- tree_mod %>% # parsnip model fit(test ~ metascore + imdb_rating, # a formula data = bechdel # dataframe ) ``` --- class: center <img src="images/predicting/predicting.001.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.003.jpeg" width="80%" style="display: block; margin: auto;" /> --- ## `predict()` Use a fitted model to predict new `y` values from data. Returns a tibble. ```r predict(tree_fit, new_data = bechdel_new) ``` --- ```r tree_fit %>% predict(new_data = bechdel_new) ## # A tibble: 3 Ã— 1 ## .pred_class ## <fct> ## 1 Pass ## 2 Pass ## 3 Pass ``` --- ## Axiom The best way to measure a model&#39;s performance at predicting new data is to .display[predict new data]. --- ## Data splitting <img src="index_files/figure-html/all-split-1.png" width="80%" style="display: block; margin: auto;" /> --- class: inverse, middle # â™»ï¸ Resample models with `rsample` --- ## `rsample` <iframe src="https://tidymodels.github.io/rsample/" width="100%" height="400px" data-external="1"></iframe> --- ## `initial_split()*` "Splits" data randomly into a single testing and a single training set. ```r initial_split(data, prop = 3/4) ``` .footnote[`*` from `rsample`] --- ```r bechdel_split <- initial_split(data = bechdel, strata = test, prop = 3/4) bechdel_split ## <Training/Testing/Total> ## <1045/349/1394> ``` --- ## `training()` and `testing()*` Extract training and testing sets from an `rsplit` ```r training(bechdel_split) testing(bechdel_split) ``` .footnote[`*` from `rsample`] --- ```r bechdel_train <- training(bechdel_split) bechdel_train ## # A tibble: 1,045 Ã— 10 ## year title test budgeâ€¦Â¹ domgrâ€¦Â² intgrâ€¦Â³ rated metasâ€¦â´ ## <dbl> <chr> <fct> <dbl> <dbl> <dbl> <chr> <dbl> ## 1 2013 12 Yeaâ€¦ Fail 2 5.31 15.9 R 97 ## 2 2013 2 Guns Fail 6.1 7.56 13.2 R 55 ## 3 2013 42 Fail 4 9.50 9.50 PG-13 62 ## 4 2013 47 Ronâ€¦ Fail 22.5 3.84 14.6 PG-13 29 ## 5 2013 A Goodâ€¦ Fail 9.2 6.73 30.4 R 28 ## 6 2013 After â€¦ Fail 13 6.05 24.4 PG-13 33 ## 7 2013 Cloudyâ€¦ Fail 7.8 12.0 27.2 PG 59 ## 8 2013 Don Jon Fail 0.55 2.45 2.64 R 66 ## 9 2013 Escapeâ€¦ Fail 7 2.52 10.4 R 49 ## 10 2013 Gangstâ€¦ Fail 6 4.60 10.4 R 40 ## # â€¦ with 1,035 more rows, 2 more variables: ## # imdb_rating <dbl>, genre <chr>, and abbreviated ## # variable names Â¹â€‹budget_2013, Â²â€‹domgross_2013, ## # Â³â€‹intgross_2013, â´â€‹metascore ## # â„¹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names ``` --- class: center <img src="images/predicting/predicting.001.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.003.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.004.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.006.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.007.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.008.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.009.jpeg" width="80%" style="display: block; margin: auto;" /> --- ## Your turn 2 Fill in the blanks. Use `initial_split()`, `training()`, and `testing()` to: 1. Split **bechdel** into training and test sets. Save the rsplit! 2. Extract the training data and fit your classification tree model. 3. Predict the testing data, and save the true `test` values. 4. Measure the accuracy of your model with your test set. Keep `set.seed(100)` at the start of your code. <div class="countdown" id="timer_62f3ebcf" style="right:0;bottom:0;" data-warnwhen="0"> <code class="countdown-time"><span class="countdown-digits minutes">04</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code> </div> --- ```r set.seed(100) # Important! bechdel_split <- initial_split(bechdel, strata = test, prop = 3/4) bechdel_train <- training(bechdel_split) bechdel_test <- testing(bechdel_split) tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel_train) %>% predict(new_data = bechdel_test) %>% mutate(true_bechdel = bechdel_test$test) %>% accuracy(truth = true_bechdel, estimate = .pred_class) ``` --- class: inverse, middle # Goal of Machine Learning ## ğŸ¯ generate accurate predictions --- ## Axiom Better Model = Better Predictions (Lower error rate) --- ## `accuracy()*` Calculates the accuracy based on two columns in a dataframe: The .display[truth]: `\({y}_i\)` The predicted .display[estimate]: `\(\hat{y}_i\)` ```r accuracy(data, truth, estimate) ``` .footnote[`*` from `yardstick`] --- ```r tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel_train) %>% predict(new_data = bechdel_test) %>% mutate(true_bechdel = bechdel_test$test) %>% * accuracy(truth = true_bechdel, estimate = .pred_class) ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.547 ``` --- class: center <img src="images/predicting/predicting.006.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.007.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.008.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.009.jpeg" width="80%" style="display: block; margin: auto;" /> --- ## Your Turn 3 What would happen if you repeated this process? Would you get the same answers? Note your accuracy from above. Then change your seed number and rerun just the last code chunk above. Do you get the same answer? Try it a few times with a few different seeds. <div class="countdown" id="timer_62f3e974" style="right:0;bottom:0;" data-warnwhen="0"> <code class="countdown-time"><span class="countdown-digits minutes">02</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code> </div> --- .pull-left[ ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.553 ``` ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.559 ``` ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.567 ``` ] -- .pull-right[ ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.539 ``` ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.504 ``` ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.550 ``` ] --- ## Quiz Why is the new estimate different? --- ## Data Splitting -- <img src="index_files/figure-html/unnamed-chunk-74-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-75-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-76-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-77-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-78-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-79-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-80-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-81-1.png" width="80%" style="display: block; margin: auto;" /> --- <img src="index_files/figure-html/unnamed-chunk-82-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-83-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-84-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-85-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-86-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-87-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-88-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-89-1.png" width="80%" style="display: block; margin: auto;" /> -- .right[Mean accuracy] --- ## Resampling Let&#39;s resample 10 times then compute the mean of the results... --- ```r acc %>% tibble::enframe(name = "accuracy") ## # A tibble: 10 Ã— 2 ## accuracy value ## <int> <dbl> ## 1 1 0.550 ## 2 2 0.625 ## 3 3 0.599 ## 4 4 0.587 ## 5 5 0.605 ## 6 6 0.593 ## 7 7 0.542 ## 8 8 0.530 ## 9 9 0.564 ## 10 10 0.599 mean(acc) ## [1] 0.5793696 ``` --- ## Guess Which do you think is a better estimate? The best result or the mean of the results? Why? --- ## But also... Fit with .display[training set] Predict with .display[testing set] -- Rinse and repeat? --- ## There has to be a better way... ```r acc <- vector(length = 10, mode = "double") for (i in 1:10) { new_split <- initial_split(bechdel) new_train <- training(new_split) new_test <- testing(new_split) acc[i] <- lr_mod %>% fit(test ~ metascore + imdb_rating, data = new_train) %>% predict(new_test) %>% mutate(truth = new_test$test) %>% accuracy(truth, .pred_class) %>% pull(.estimate) } ``` --- background-image: url(images/diamonds.jpg) background-size: contain background-position: left class: middle, center background-color: #f5f5f5 .pull-right[ ## The .display[testing set] is precious... ## we can only use it once! ] --- background-image: url(images/diamonds.jpg) background-size: contain background-position: left class: middle, center background-color: #f5f5f5 .pull-right[ ## How can we use the training set to compare, evaluate, and tune models? ] --- class: middle <img src="https://www.tidymodels.org/start/resampling/img/resampling.svg" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide2.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide3.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide4.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide5.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide6.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide7.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide8.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide9.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide10.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide11.png" width="80%" style="display: block; margin: auto;" /> --- ## V-fold cross-validation ```r vfold_cv(data, v = 10, ...) ``` --- exclude: true --- ## Guess How many times does an observation/row appear in the assessment set? <img src="index_files/figure-html/vfold-tiles-1.png" width="80%" style="display: block; margin: auto;" /> --- <img src="index_files/figure-html/unnamed-chunk-105-1.png" width="80%" style="display: block; margin: auto;" /> --- ## Quiz If we use 10 folds, which percent of our data will end up in the training set and which percent in the testing set for each fold? -- 90% - training 10% - test --- ## Your Turn 4 Run the code below. What does it return? ```r set.seed(100) bechdel_folds <- vfold_cv(data = bechdel_train, v = 10, strata = test) bechdel_folds ``` <div class="countdown" id="timer_62f3e9e7" style="right:0;bottom:0;" data-warnwhen="0"> <code class="countdown-time"><span class="countdown-digits minutes">01</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code> </div> --- ```r set.seed(100) bechdel_folds <- vfold_cv(data = bechdel_train, v = 10, strata = test) bechdel_folds ## # 10-fold cross-validation using stratification ## # A tibble: 10 Ã— 2 ## splits id ## <list> <chr> ## 1 <split [940/105]> Fold01 ## 2 <split [940/105]> Fold02 ## 3 <split [940/105]> Fold03 ## 4 <split [940/105]> Fold04 ## 5 <split [940/105]> Fold05 ## 6 <split [940/105]> Fold06 ## 7 <split [941/104]> Fold07 ## 8 <split [941/104]> Fold08 ## 9 <split [941/104]> Fold09 ## 10 <split [942/103]> Fold10 ``` --- ## We need a new way to fit ```r split1 <- bechdel_folds %>% pluck("splits", 1) split1_train <- training(split1) split1_test <- testing(split1) tree_mod %>% fit(test ~ ., data = split1_train) %>% predict(split1_test) %>% mutate(truth = split1_test$test) %>% accuracy(truth, .pred_class) # rinse and repeat split2 <- ... ``` --- ## `fit_resamples()` Trains and tests a resampled model. ```r tree_mod %>% fit_resamples( test ~ metascore + imdb_rating, resamples = bechdel_folds ) ``` --- ```r tree_mod %>% fit_resamples( test ~ metascore + imdb_rating, resamples = bechdel_folds ) ## # Resampling results ## # 10-fold cross-validation using stratification ## # A tibble: 10 Ã— 4 ## splits id .metrics .notes ## <list> <chr> <list> <list> ## 1 <split [940/105]> Fold01 <tibble [2 Ã— 4]> <tibble> ## 2 <split [940/105]> Fold02 <tibble [2 Ã— 4]> <tibble> ## 3 <split [940/105]> Fold03 <tibble [2 Ã— 4]> <tibble> ## 4 <split [940/105]> Fold04 <tibble [2 Ã— 4]> <tibble> ## 5 <split [940/105]> Fold05 <tibble [2 Ã— 4]> <tibble> ## 6 <split [940/105]> Fold06 <tibble [2 Ã— 4]> <tibble> ## 7 <split [941/104]> Fold07 <tibble [2 Ã— 4]> <tibble> ## 8 <split [941/104]> Fold08 <tibble [2 Ã— 4]> <tibble> ## 9 <split [941/104]> Fold09 <tibble [2 Ã— 4]> <tibble> ## 10 <split [942/103]> Fold10 <tibble [2 Ã— 4]> <tibble> ``` --- ## `collect_metrics()` Unnest the metrics column from a tidymodels `fit_resamples()` ```r _results %>% collect_metrics(summarize = TRUE) ``` -- .footnote[`TRUE` is actually the default; averages across folds] --- ```r tree_mod %>% fit_resamples( test ~ metascore + imdb_rating, resamples = bechdel_folds ) %>% collect_metrics(summarize = FALSE) ## # A tibble: 20 Ã— 5 ## id .metric .estimator .estimate .config ## <chr> <chr> <chr> <dbl> <chr> ## 1 Fold01 accuracy binary 0.610 Preprocessor1_Model1 ## 2 Fold01 roc_auc binary 0.587 Preprocessor1_Model1 ## 3 Fold02 accuracy binary 0.438 Preprocessor1_Model1 ## 4 Fold02 roc_auc binary 0.434 Preprocessor1_Model1 ## 5 Fold03 accuracy binary 0.6 Preprocessor1_Model1 ## 6 Fold03 roc_auc binary 0.570 Preprocessor1_Model1 ## 7 Fold04 accuracy binary 0.562 Preprocessor1_Model1 ## 8 Fold04 roc_auc binary 0.547 Preprocessor1_Model1 ## 9 Fold05 accuracy binary 0.6 Preprocessor1_Model1 ## 10 Fold05 roc_auc binary 0.572 Preprocessor1_Model1 ## # â€¦ with 10 more rows ## # â„¹ Use `print(n = ...)` to see more rows ``` --- ## 10-fold CV - 10 different analysis/assessment sets - 10 different models (trained on .display[analysis] sets) - 10 different sets of performance statistics (on .display[assessment] sets) --- ## Your Turn 5 Modify the code below to use `fit_resamples` and `bechdel_folds` to cross-validate the classification tree model. What is the ROC AUC that you collect at the end? ```r set.seed(100) tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel_train) %>% predict(new_data = bechdel_test) %>% mutate(true_bechdel = bechdel_test$test) %>% accuracy(truth = true_bechdel, estimate = .pred_class) ``` <div class="countdown" id="timer_62f3ebc2" style="right:0;bottom:0;" data-warnwhen="0"> <code class="countdown-time"><span class="countdown-digits minutes">03</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>'><meta property="og:url" content="https://cfssmacss.netlify.app/slides/machine-learning/"><meta property="og:site_name" content="CFSS: Computing for the Social Sciences"><meta property="og:title" content="CFSS: Computing for the Social Sciences"><meta property="og:description" content='<!DOCTYPE html> An introduction to machine learning class: center, middle, inverse, title-slide .title[ # An introduction to machine learning ] .author[ ### INFO 5940 Cornell University ] --- class: inverse, middle # What is machine learning? --- class: middle <img src="../../../../../../../../img/amazon-recommendations.png" width="80%" style="display: block; margin: auto;" /> --- class: middle <img src="https://miro.medium.com/max/1400/1*j5aWfH9t1_EZPJC92CJ7oQ.png" width="80%" style="display: block; margin: auto;" /> .footnote[https://medium.com/tmobile-tech/small-data-big-value-f783ceca4fdb] --- class: middle <img src="https://techcrunch.com/wp-content/uploads/2017/12/facebook-facial-recognition-photo-review.png?w=730&amp;crop=1" width="80%" style="display: block; margin: auto;" /> --- class: middle, inverse # What is machine learning? --- class: middle, center <img src="https://imgs.xkcd.com/comics/machine_learning.png" width="40%" style="display: block; margin: auto;" /> --- class: middle <img src="images/intro.002.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: middle <img src="images/intro.003.jpeg" width="80%" style="display: block; margin: auto;" /> --- ## Types of machine learning - Supervised - Unsupervised --- <img src="images/all-of-ml.jpg" width="80%" style="display: block; margin: auto;" /> .footnote[Credit: <https://vas3k.com/blog/machine_learning/>] --- ## Examples of supervised learning - Ad click prediction - Supreme Court decision making - Police misconduct prediction --- ## Two modes -- .pull-left[ ### Classification Is your hospital in a state of crisis care? ] -- .pull-left[ ### Regression What percentage of hospital beds are occupied? ] --- ## Two cultures .pull-left[ ### Statistics - model first - inference emphasis ] .pull-right[ ### Machine Learning - data first - prediction emphasis ] --- name: train-love background-image: url(images/train.jpg) background-size: contain background-color: #f6f6f6 --- template: train-love class: center, top # Statistics --- template: train-love class: bottom > *"Statisticians, like artists, have the bad habit of falling in love with their models."* > > &amp;mdash; George Box --- ## Predictive modeling ```r library(tidymodels) ``` ``` ## â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.0.0 â”€â”€ ``` ``` ## âœ” broom 1.0.0 âœ” recipes 1.0.1 ## âœ” dials 1.0.0 âœ” rsample 1.1.0 ## âœ” dplyr 1.0.9 âœ” tibble 3.1.8 ## âœ” ggplot2 3.3.6 âœ” tidyr 1.2.0 ## âœ” infer 1.0.2 âœ” tune 1.0.0 ## âœ” modeldata 1.0.0 âœ” workflows 1.0.0 ## âœ” parsnip 1.0.0 âœ” workflowsets 1.0.0 ## âœ” purrr 0.3.4 âœ” yardstick 1.0.0 ``` ``` ## â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€ ## âœ– purrr::discard() masks scales::discard() ## âœ– dplyr::filter() masks stats::filter() ## âœ– dplyr::lag() masks stats::lag() ## âœ– recipes::step() masks stats::step() ## â€¢ Search for functions across packages at https://www.tidymodels.org/find/ ``` --- ## Bechdel Test <img src="https://fivethirtyeight.com/wp-content/uploads/2014/04/hickey-bechdel-11.png" width="60%" style="display: block; margin: auto;" /> .footnote[Source:[FiveThirtyEight](https://fivethirtyeight.com/features/the-dollar-and-cents-case-against-hollywoods-exclusion-of-women/)] --- ## Bechdel Test 1. It has to have at least two [named] women in it 1. Who talk to each other 1. About something besides a man -- ```r library(rcis) data("bechdel") glimpse(bechdel) ## Rows: 1,394 ## Columns: 10 ## $ year <dbl> 2013, 2013, 2013, 2013, 2013, 2013, â€¦ ## $ title <chr> "12 Years a Slave", "2 Guns", "42", â€¦ ## $ test <fct> Fail, Fail, Fail, Fail, Fail, Pass, â€¦ ## $ budget_2013 <dbl> 2.00, 6.10, 4.00, 22.50, 9.20, 1.20,â€¦ ## $ domgross_2013 <dbl> 5.310703, 7.561246, 9.502021, 3.8362â€¦ ## $ intgross_2013 <dbl> 15.860703, 13.249301, 9.502021, 14.5â€¦ ## $ rated <chr> "R", "R", "PG-13", "PG-13", "R", "R"â€¦ ## $ metascore <dbl> 97, 55, 62, 29, 28, 55, 48, 33, 90, â€¦ ## $ imdb_rating <dbl> 8.3, 6.8, 7.6, 6.6, 5.4, 7.8, 5.7, 5â€¦ ## $ genre <chr> "Biography", "Action", "Biography", â€¦ ``` --- ## Bechdel test data - N = 1394 - 1 categorical outcome: `test` - 9 predictors --- class: middle <img src="index_files/figure-html/unnamed-chunk-11-1.png" width="80%" style="display: block; margin: auto;" /> --- class: inverse, middle # What is the goal of machine learning? -- ## Build .white[models] that -- ## generate .white[accurate predictions] -- ## for .white[future, yet-to-be-seen data]. -- .footnote[Max Kuhn & Kjell Johnston, http://www.feat.engineering/] --- ## Machine learning We&#39;ll use this goal to drive learning of 3 core `tidymodels` packages: - `parsnip` - `yardstick` - `rsample` --- class: inverse, middle ## ğŸ”¨ Build models with `parsnip` --- class: middle, center, frame ## parsnip <iframe src="https://parsnip.tidymodels.org" width="100%" height="400px" data-external="1"></iframe> --- ## `glm()` ```r glm(test ~ metascore, family = binomial, data = bechdel) ## ## Call: glm(formula = test ~ metascore, family = binomial, data = bechdel) ## ## Coefficients: ## (Intercept) metascore ## 0.052274 -0.004563 ## ## Degrees of Freedom: 1393 Total (i.e. Null); 1392 Residual ## Null Deviance:	1916 ## Residual Deviance: 1914 AIC: 1918 ``` --- class: center <img src="images/predicting/predicting.001.jpeg" width="80%" style="display: block; margin: auto;" /> --- ## To specify a model with `parsnip` 1. Pick a model 1. Set the engine 1. Set the mode (if needed) --- ## To specify a model with `parsnip` ```r logistic_reg() %>% set_engine("glm") %>% set_mode("classification") ## Logistic Regression Model Specification (classification) ## ## Computational engine: glm ``` --- ## To specify a model with `parsnip` ```r decision_tree() %>% set_engine("C5.0") %>% set_mode("classification") ## Decision Tree Model Specification (classification) ## ## Computational engine: C5.0 ``` --- ## To specify a model with `parsnip` ```r nearest_neighbor() %>% set_engine("kknn") %>% set_mode("classification") ## K-Nearest Neighbor Model Specification (classification) ## ## Computational engine: kknn ``` --- ## 1\. Pick a model All available models are listed at <https://www.tidymodels.org/find/parsnip/> <iframe src="https://www.tidymodels.org/find/parsnip/" width="100%" height="400px" data-external="1"></iframe> --- ## `logistic_reg()` Specifies a model that uses logistic regression ```r logistic_reg(penalty = NULL, mixture = NULL) ``` --- ## `logistic_reg()` Specifies a model that uses logistic regression ```r logistic_reg( mode = "classification", # "default" mode, if exists penalty = NULL, # model hyper-parameter mixture = NULL # model hyper-parameter ) ``` --- ## `set_engine()` Adds an engine to power or implement the model. ```r logistic_reg() %>% set_engine(engine = "glm") ``` --- ## `set_mode()` Sets the class of problem the model will solve, which influences which output is collected. Not necessary if mode is set in Step 1. ```r logistic_reg() %>% set_mode(mode = "classification") ``` --- class: your-turn ## Your turn 1 Run the chunk in your .Rmd and look at the output. Then, copy/paste the code and edit to create: + a decision tree model for classification + that uses the `C5.0` engine. Save it as `tree_mod` and look at the object. What is different about the output? *Hint: you&#39;ll need https://www.tidymodels.org/find/parsnip/* <div class="countdown" id="timer_62f3ecba" style="right:0;bottom:0;" data-warnwhen="0"> <code class="countdown-time"><span class="countdown-digits minutes">03</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code> </div> --- ```r lr_mod ## Logistic Regression Model Specification (classification) ## ## Computational engine: glm tree_mod <- decision_tree() %>% set_engine(engine = "C5.0") %>% set_mode("classification") tree_mod ## Decision Tree Model Specification (classification) ## ## Computational engine: C5.0 ``` --- class: inverse, middle ## Now we&#39;ve built a model. -- ## But, how do we .white[use] a model? -- ## First - what does it mean to use a model? --- class: inverse, middle, center ![](https://media.giphy.com/media/fhAwk4DnqNgw8/giphy.gif) Statistical models learn from the data. Many learn model parameters, which *can* be useful as values for inference and interpretation. --- ## A fitted model .pull-left[ ```r lr_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel) %>% broom::tidy() ## # A tibble: 3 Ã— 5 ## term estimate std.error statistic p.value ## <chr> <dbl> <dbl> <dbl> <dbl> ## 1 (Intercept) 2.70 0.436 6.20 5.64e-10 ## 2 metascore 0.0202 0.00481 4.20 2.66e- 5 ## 3 imdb_rating -0.606 0.0889 -6.82 8.87e-12 ``` ] .pull-right[ <img src="index_files/figure-html/unnamed-chunk-27-1.png" width="80%" style="display: block; margin: auto;" /> ] --- ## "All models are wrong, but some are useful" <img src="index_files/figure-html/unnamed-chunk-29-1.png" width="80%" style="display: block; margin: auto;" /> --- ## "All models are wrong, but some are useful" <img src="index_files/figure-html/unnamed-chunk-31-1.png" width="80%" style="display: block; margin: auto;" /> --- ## Predict new data ```r bechdel_new <- tibble(metascore = c(40, 50, 60), imdb_rating = c(6, 6, 6), test = c("Fail", "Fail", "Pass")) %>% mutate(test = factor(test, levels = c("Fail", "Pass"))) bechdel_new ## # A tibble: 3 Ã— 3 ## metascore imdb_rating test ## <dbl> <dbl> <fct> ## 1 40 6 Fail ## 2 50 6 Fail ## 3 60 6 Pass ``` --- ## Predict old data ```r tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel) %>% predict(new_data = bechdel) %>% mutate(true_bechdel = bechdel$test) %>% accuracy(truth = true_bechdel, estimate = .pred_class) ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.580 ``` --- ## Predict new data .pull-left[ ### out with the old... ```r tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel) %>% predict(new_data = bechdel) %>% mutate(true_bechdel = bechdel$test) %>% accuracy(truth = true_bechdel, estimate = .pred_class) ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.580 ``` ] .pull-right[ ### in with the ğŸ†• ```r tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel) %>% * predict(new_data = bechdel_new) %>% * mutate(true_bechdel = bechdel_new$test) %>% accuracy(truth = true_bechdel, estimate = .pred_class) ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.333 ``` ] --- ## `fit()` Train a model by fitting a model. Returns a parsnip model fit. ```r fit(tree_mod, test ~ metascore + imdb_rating, data = bechdel) ``` --- ## `fit()` Train a model by fitting a model. Returns a parsnip model fit. ```r tree_mod %>% # parsnip model fit(test ~ metascore + imdb_rating, # a formula data = bechdel # dataframe ) ``` --- ## `fit()` Train a model by fitting a model. Returns a parsnip model fit. ```r tree_fit <- tree_mod %>% # parsnip model fit(test ~ metascore + imdb_rating, # a formula data = bechdel # dataframe ) ``` --- class: center <img src="images/predicting/predicting.001.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.003.jpeg" width="80%" style="display: block; margin: auto;" /> --- ## `predict()` Use a fitted model to predict new `y` values from data. Returns a tibble. ```r predict(tree_fit, new_data = bechdel_new) ``` --- ```r tree_fit %>% predict(new_data = bechdel_new) ## # A tibble: 3 Ã— 1 ## .pred_class ## <fct> ## 1 Pass ## 2 Pass ## 3 Pass ``` --- ## Axiom The best way to measure a model&#39;s performance at predicting new data is to .display[predict new data]. --- ## Data splitting <img src="index_files/figure-html/all-split-1.png" width="80%" style="display: block; margin: auto;" /> --- class: inverse, middle # â™»ï¸ Resample models with `rsample` --- ## `rsample` <iframe src="https://tidymodels.github.io/rsample/" width="100%" height="400px" data-external="1"></iframe> --- ## `initial_split()*` "Splits" data randomly into a single testing and a single training set. ```r initial_split(data, prop = 3/4) ``` .footnote[`*` from `rsample`] --- ```r bechdel_split <- initial_split(data = bechdel, strata = test, prop = 3/4) bechdel_split ## <Training/Testing/Total> ## <1045/349/1394> ``` --- ## `training()` and `testing()*` Extract training and testing sets from an `rsplit` ```r training(bechdel_split) testing(bechdel_split) ``` .footnote[`*` from `rsample`] --- ```r bechdel_train <- training(bechdel_split) bechdel_train ## # A tibble: 1,045 Ã— 10 ## year title test budgeâ€¦Â¹ domgrâ€¦Â² intgrâ€¦Â³ rated metasâ€¦â´ ## <dbl> <chr> <fct> <dbl> <dbl> <dbl> <chr> <dbl> ## 1 2013 12 Yeaâ€¦ Fail 2 5.31 15.9 R 97 ## 2 2013 2 Guns Fail 6.1 7.56 13.2 R 55 ## 3 2013 42 Fail 4 9.50 9.50 PG-13 62 ## 4 2013 47 Ronâ€¦ Fail 22.5 3.84 14.6 PG-13 29 ## 5 2013 A Goodâ€¦ Fail 9.2 6.73 30.4 R 28 ## 6 2013 After â€¦ Fail 13 6.05 24.4 PG-13 33 ## 7 2013 Cloudyâ€¦ Fail 7.8 12.0 27.2 PG 59 ## 8 2013 Don Jon Fail 0.55 2.45 2.64 R 66 ## 9 2013 Escapeâ€¦ Fail 7 2.52 10.4 R 49 ## 10 2013 Gangstâ€¦ Fail 6 4.60 10.4 R 40 ## # â€¦ with 1,035 more rows, 2 more variables: ## # imdb_rating <dbl>, genre <chr>, and abbreviated ## # variable names Â¹â€‹budget_2013, Â²â€‹domgross_2013, ## # Â³â€‹intgross_2013, â´â€‹metascore ## # â„¹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names ``` --- class: center <img src="images/predicting/predicting.001.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.003.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.004.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.006.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.007.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.008.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.009.jpeg" width="80%" style="display: block; margin: auto;" /> --- ## Your turn 2 Fill in the blanks. Use `initial_split()`, `training()`, and `testing()` to: 1. Split **bechdel** into training and test sets. Save the rsplit! 2. Extract the training data and fit your classification tree model. 3. Predict the testing data, and save the true `test` values. 4. Measure the accuracy of your model with your test set. Keep `set.seed(100)` at the start of your code. <div class="countdown" id="timer_62f3ebcf" style="right:0;bottom:0;" data-warnwhen="0"> <code class="countdown-time"><span class="countdown-digits minutes">04</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code> </div> --- ```r set.seed(100) # Important! bechdel_split <- initial_split(bechdel, strata = test, prop = 3/4) bechdel_train <- training(bechdel_split) bechdel_test <- testing(bechdel_split) tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel_train) %>% predict(new_data = bechdel_test) %>% mutate(true_bechdel = bechdel_test$test) %>% accuracy(truth = true_bechdel, estimate = .pred_class) ``` --- class: inverse, middle # Goal of Machine Learning ## ğŸ¯ generate accurate predictions --- ## Axiom Better Model = Better Predictions (Lower error rate) --- ## `accuracy()*` Calculates the accuracy based on two columns in a dataframe: The .display[truth]: `\({y}_i\)` The predicted .display[estimate]: `\(\hat{y}_i\)` ```r accuracy(data, truth, estimate) ``` .footnote[`*` from `yardstick`] --- ```r tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel_train) %>% predict(new_data = bechdel_test) %>% mutate(true_bechdel = bechdel_test$test) %>% * accuracy(truth = true_bechdel, estimate = .pred_class) ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.547 ``` --- class: center <img src="images/predicting/predicting.006.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.007.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.008.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.009.jpeg" width="80%" style="display: block; margin: auto;" /> --- ## Your Turn 3 What would happen if you repeated this process? Would you get the same answers? Note your accuracy from above. Then change your seed number and rerun just the last code chunk above. Do you get the same answer? Try it a few times with a few different seeds. <div class="countdown" id="timer_62f3e974" style="right:0;bottom:0;" data-warnwhen="0"> <code class="countdown-time"><span class="countdown-digits minutes">02</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code> </div> --- .pull-left[ ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.553 ``` ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.559 ``` ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.567 ``` ] -- .pull-right[ ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.539 ``` ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.504 ``` ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.550 ``` ] --- ## Quiz Why is the new estimate different? --- ## Data Splitting -- <img src="index_files/figure-html/unnamed-chunk-74-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-75-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-76-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-77-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-78-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-79-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-80-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-81-1.png" width="80%" style="display: block; margin: auto;" /> --- <img src="index_files/figure-html/unnamed-chunk-82-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-83-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-84-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-85-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-86-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-87-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-88-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-89-1.png" width="80%" style="display: block; margin: auto;" /> -- .right[Mean accuracy] --- ## Resampling Let&#39;s resample 10 times then compute the mean of the results... --- ```r acc %>% tibble::enframe(name = "accuracy") ## # A tibble: 10 Ã— 2 ## accuracy value ## <int> <dbl> ## 1 1 0.550 ## 2 2 0.625 ## 3 3 0.599 ## 4 4 0.587 ## 5 5 0.605 ## 6 6 0.593 ## 7 7 0.542 ## 8 8 0.530 ## 9 9 0.564 ## 10 10 0.599 mean(acc) ## [1] 0.5793696 ``` --- ## Guess Which do you think is a better estimate? The best result or the mean of the results? Why? --- ## But also... Fit with .display[training set] Predict with .display[testing set] -- Rinse and repeat? --- ## There has to be a better way... ```r acc <- vector(length = 10, mode = "double") for (i in 1:10) { new_split <- initial_split(bechdel) new_train <- training(new_split) new_test <- testing(new_split) acc[i] <- lr_mod %>% fit(test ~ metascore + imdb_rating, data = new_train) %>% predict(new_test) %>% mutate(truth = new_test$test) %>% accuracy(truth, .pred_class) %>% pull(.estimate) } ``` --- background-image: url(images/diamonds.jpg) background-size: contain background-position: left class: middle, center background-color: #f5f5f5 .pull-right[ ## The .display[testing set] is precious... ## we can only use it once! ] --- background-image: url(images/diamonds.jpg) background-size: contain background-position: left class: middle, center background-color: #f5f5f5 .pull-right[ ## How can we use the training set to compare, evaluate, and tune models? ] --- class: middle <img src="https://www.tidymodels.org/start/resampling/img/resampling.svg" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide2.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide3.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide4.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide5.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide6.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide7.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide8.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide9.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide10.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide11.png" width="80%" style="display: block; margin: auto;" /> --- ## V-fold cross-validation ```r vfold_cv(data, v = 10, ...) ``` --- exclude: true --- ## Guess How many times does an observation/row appear in the assessment set? <img src="index_files/figure-html/vfold-tiles-1.png" width="80%" style="display: block; margin: auto;" /> --- <img src="index_files/figure-html/unnamed-chunk-105-1.png" width="80%" style="display: block; margin: auto;" /> --- ## Quiz If we use 10 folds, which percent of our data will end up in the training set and which percent in the testing set for each fold? -- 90% - training 10% - test --- ## Your Turn 4 Run the code below. What does it return? ```r set.seed(100) bechdel_folds <- vfold_cv(data = bechdel_train, v = 10, strata = test) bechdel_folds ``` <div class="countdown" id="timer_62f3e9e7" style="right:0;bottom:0;" data-warnwhen="0"> <code class="countdown-time"><span class="countdown-digits minutes">01</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code> </div> --- ```r set.seed(100) bechdel_folds <- vfold_cv(data = bechdel_train, v = 10, strata = test) bechdel_folds ## # 10-fold cross-validation using stratification ## # A tibble: 10 Ã— 2 ## splits id ## <list> <chr> ## 1 <split [940/105]> Fold01 ## 2 <split [940/105]> Fold02 ## 3 <split [940/105]> Fold03 ## 4 <split [940/105]> Fold04 ## 5 <split [940/105]> Fold05 ## 6 <split [940/105]> Fold06 ## 7 <split [941/104]> Fold07 ## 8 <split [941/104]> Fold08 ## 9 <split [941/104]> Fold09 ## 10 <split [942/103]> Fold10 ``` --- ## We need a new way to fit ```r split1 <- bechdel_folds %>% pluck("splits", 1) split1_train <- training(split1) split1_test <- testing(split1) tree_mod %>% fit(test ~ ., data = split1_train) %>% predict(split1_test) %>% mutate(truth = split1_test$test) %>% accuracy(truth, .pred_class) # rinse and repeat split2 <- ... ``` --- ## `fit_resamples()` Trains and tests a resampled model. ```r tree_mod %>% fit_resamples( test ~ metascore + imdb_rating, resamples = bechdel_folds ) ``` --- ```r tree_mod %>% fit_resamples( test ~ metascore + imdb_rating, resamples = bechdel_folds ) ## # Resampling results ## # 10-fold cross-validation using stratification ## # A tibble: 10 Ã— 4 ## splits id .metrics .notes ## <list> <chr> <list> <list> ## 1 <split [940/105]> Fold01 <tibble [2 Ã— 4]> <tibble> ## 2 <split [940/105]> Fold02 <tibble [2 Ã— 4]> <tibble> ## 3 <split [940/105]> Fold03 <tibble [2 Ã— 4]> <tibble> ## 4 <split [940/105]> Fold04 <tibble [2 Ã— 4]> <tibble> ## 5 <split [940/105]> Fold05 <tibble [2 Ã— 4]> <tibble> ## 6 <split [940/105]> Fold06 <tibble [2 Ã— 4]> <tibble> ## 7 <split [941/104]> Fold07 <tibble [2 Ã— 4]> <tibble> ## 8 <split [941/104]> Fold08 <tibble [2 Ã— 4]> <tibble> ## 9 <split [941/104]> Fold09 <tibble [2 Ã— 4]> <tibble> ## 10 <split [942/103]> Fold10 <tibble [2 Ã— 4]> <tibble> ``` --- ## `collect_metrics()` Unnest the metrics column from a tidymodels `fit_resamples()` ```r _results %>% collect_metrics(summarize = TRUE) ``` -- .footnote[`TRUE` is actually the default; averages across folds] --- ```r tree_mod %>% fit_resamples( test ~ metascore + imdb_rating, resamples = bechdel_folds ) %>% collect_metrics(summarize = FALSE) ## # A tibble: 20 Ã— 5 ## id .metric .estimator .estimate .config ## <chr> <chr> <chr> <dbl> <chr> ## 1 Fold01 accuracy binary 0.610 Preprocessor1_Model1 ## 2 Fold01 roc_auc binary 0.587 Preprocessor1_Model1 ## 3 Fold02 accuracy binary 0.438 Preprocessor1_Model1 ## 4 Fold02 roc_auc binary 0.434 Preprocessor1_Model1 ## 5 Fold03 accuracy binary 0.6 Preprocessor1_Model1 ## 6 Fold03 roc_auc binary 0.570 Preprocessor1_Model1 ## 7 Fold04 accuracy binary 0.562 Preprocessor1_Model1 ## 8 Fold04 roc_auc binary 0.547 Preprocessor1_Model1 ## 9 Fold05 accuracy binary 0.6 Preprocessor1_Model1 ## 10 Fold05 roc_auc binary 0.572 Preprocessor1_Model1 ## # â€¦ with 10 more rows ## # â„¹ Use `print(n = ...)` to see more rows ``` --- ## 10-fold CV - 10 different analysis/assessment sets - 10 different models (trained on .display[analysis] sets) - 10 different sets of performance statistics (on .display[assessment] sets) --- ## Your Turn 5 Modify the code below to use `fit_resamples` and `bechdel_folds` to cross-validate the classification tree model. What is the ROC AUC that you collect at the end? ```r set.seed(100) tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel_train) %>% predict(new_data = bechdel_test) %>% mutate(true_bechdel = bechdel_test$test) %>% accuracy(truth = true_bechdel, estimate = .pred_class) ``` <div class="countdown" id="timer_62f3ebc2" style="right:0;bottom:0;" data-warnwhen="0"> <code class="countdown-time"><span class="countdown-digits minutes">03</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>'><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="slides"><meta itemprop=name content="CFSS: Computing for the Social Sciences"><meta itemprop=description content='<!DOCTYPE html> An introduction to machine learning class: center, middle, inverse, title-slide .title[ # An introduction to machine learning ] .author[ ### INFO 5940 Cornell University ] --- class: inverse, middle # What is machine learning? --- class: middle <img src="../../../../../../../../img/amazon-recommendations.png" width="80%" style="display: block; margin: auto;" /> --- class: middle <img src="https://miro.medium.com/max/1400/1*j5aWfH9t1_EZPJC92CJ7oQ.png" width="80%" style="display: block; margin: auto;" /> .footnote[https://medium.com/tmobile-tech/small-data-big-value-f783ceca4fdb] --- class: middle <img src="https://techcrunch.com/wp-content/uploads/2017/12/facebook-facial-recognition-photo-review.png?w=730&amp;crop=1" width="80%" style="display: block; margin: auto;" /> --- class: middle, inverse # What is machine learning? --- class: middle, center <img src="https://imgs.xkcd.com/comics/machine_learning.png" width="40%" style="display: block; margin: auto;" /> --- class: middle <img src="images/intro.002.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: middle <img src="images/intro.003.jpeg" width="80%" style="display: block; margin: auto;" /> --- ## Types of machine learning - Supervised - Unsupervised --- <img src="images/all-of-ml.jpg" width="80%" style="display: block; margin: auto;" /> .footnote[Credit: <https://vas3k.com/blog/machine_learning/>] --- ## Examples of supervised learning - Ad click prediction - Supreme Court decision making - Police misconduct prediction --- ## Two modes -- .pull-left[ ### Classification Is your hospital in a state of crisis care? ] -- .pull-left[ ### Regression What percentage of hospital beds are occupied? ] --- ## Two cultures .pull-left[ ### Statistics - model first - inference emphasis ] .pull-right[ ### Machine Learning - data first - prediction emphasis ] --- name: train-love background-image: url(images/train.jpg) background-size: contain background-color: #f6f6f6 --- template: train-love class: center, top # Statistics --- template: train-love class: bottom > *"Statisticians, like artists, have the bad habit of falling in love with their models."* > > &amp;mdash; George Box --- ## Predictive modeling ```r library(tidymodels) ``` ``` ## â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.0.0 â”€â”€ ``` ``` ## âœ” broom 1.0.0 âœ” recipes 1.0.1 ## âœ” dials 1.0.0 âœ” rsample 1.1.0 ## âœ” dplyr 1.0.9 âœ” tibble 3.1.8 ## âœ” ggplot2 3.3.6 âœ” tidyr 1.2.0 ## âœ” infer 1.0.2 âœ” tune 1.0.0 ## âœ” modeldata 1.0.0 âœ” workflows 1.0.0 ## âœ” parsnip 1.0.0 âœ” workflowsets 1.0.0 ## âœ” purrr 0.3.4 âœ” yardstick 1.0.0 ``` ``` ## â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€ ## âœ– purrr::discard() masks scales::discard() ## âœ– dplyr::filter() masks stats::filter() ## âœ– dplyr::lag() masks stats::lag() ## âœ– recipes::step() masks stats::step() ## â€¢ Search for functions across packages at https://www.tidymodels.org/find/ ``` --- ## Bechdel Test <img src="https://fivethirtyeight.com/wp-content/uploads/2014/04/hickey-bechdel-11.png" width="60%" style="display: block; margin: auto;" /> .footnote[Source:[FiveThirtyEight](https://fivethirtyeight.com/features/the-dollar-and-cents-case-against-hollywoods-exclusion-of-women/)] --- ## Bechdel Test 1. It has to have at least two [named] women in it 1. Who talk to each other 1. About something besides a man -- ```r library(rcis) data("bechdel") glimpse(bechdel) ## Rows: 1,394 ## Columns: 10 ## $ year <dbl> 2013, 2013, 2013, 2013, 2013, 2013, â€¦ ## $ title <chr> "12 Years a Slave", "2 Guns", "42", â€¦ ## $ test <fct> Fail, Fail, Fail, Fail, Fail, Pass, â€¦ ## $ budget_2013 <dbl> 2.00, 6.10, 4.00, 22.50, 9.20, 1.20,â€¦ ## $ domgross_2013 <dbl> 5.310703, 7.561246, 9.502021, 3.8362â€¦ ## $ intgross_2013 <dbl> 15.860703, 13.249301, 9.502021, 14.5â€¦ ## $ rated <chr> "R", "R", "PG-13", "PG-13", "R", "R"â€¦ ## $ metascore <dbl> 97, 55, 62, 29, 28, 55, 48, 33, 90, â€¦ ## $ imdb_rating <dbl> 8.3, 6.8, 7.6, 6.6, 5.4, 7.8, 5.7, 5â€¦ ## $ genre <chr> "Biography", "Action", "Biography", â€¦ ``` --- ## Bechdel test data - N = 1394 - 1 categorical outcome: `test` - 9 predictors --- class: middle <img src="index_files/figure-html/unnamed-chunk-11-1.png" width="80%" style="display: block; margin: auto;" /> --- class: inverse, middle # What is the goal of machine learning? -- ## Build .white[models] that -- ## generate .white[accurate predictions] -- ## for .white[future, yet-to-be-seen data]. -- .footnote[Max Kuhn & Kjell Johnston, http://www.feat.engineering/] --- ## Machine learning We&#39;ll use this goal to drive learning of 3 core `tidymodels` packages: - `parsnip` - `yardstick` - `rsample` --- class: inverse, middle ## ğŸ”¨ Build models with `parsnip` --- class: middle, center, frame ## parsnip <iframe src="https://parsnip.tidymodels.org" width="100%" height="400px" data-external="1"></iframe> --- ## `glm()` ```r glm(test ~ metascore, family = binomial, data = bechdel) ## ## Call: glm(formula = test ~ metascore, family = binomial, data = bechdel) ## ## Coefficients: ## (Intercept) metascore ## 0.052274 -0.004563 ## ## Degrees of Freedom: 1393 Total (i.e. Null); 1392 Residual ## Null Deviance:	1916 ## Residual Deviance: 1914 AIC: 1918 ``` --- class: center <img src="images/predicting/predicting.001.jpeg" width="80%" style="display: block; margin: auto;" /> --- ## To specify a model with `parsnip` 1. Pick a model 1. Set the engine 1. Set the mode (if needed) --- ## To specify a model with `parsnip` ```r logistic_reg() %>% set_engine("glm") %>% set_mode("classification") ## Logistic Regression Model Specification (classification) ## ## Computational engine: glm ``` --- ## To specify a model with `parsnip` ```r decision_tree() %>% set_engine("C5.0") %>% set_mode("classification") ## Decision Tree Model Specification (classification) ## ## Computational engine: C5.0 ``` --- ## To specify a model with `parsnip` ```r nearest_neighbor() %>% set_engine("kknn") %>% set_mode("classification") ## K-Nearest Neighbor Model Specification (classification) ## ## Computational engine: kknn ``` --- ## 1\. Pick a model All available models are listed at <https://www.tidymodels.org/find/parsnip/> <iframe src="https://www.tidymodels.org/find/parsnip/" width="100%" height="400px" data-external="1"></iframe> --- ## `logistic_reg()` Specifies a model that uses logistic regression ```r logistic_reg(penalty = NULL, mixture = NULL) ``` --- ## `logistic_reg()` Specifies a model that uses logistic regression ```r logistic_reg( mode = "classification", # "default" mode, if exists penalty = NULL, # model hyper-parameter mixture = NULL # model hyper-parameter ) ``` --- ## `set_engine()` Adds an engine to power or implement the model. ```r logistic_reg() %>% set_engine(engine = "glm") ``` --- ## `set_mode()` Sets the class of problem the model will solve, which influences which output is collected. Not necessary if mode is set in Step 1. ```r logistic_reg() %>% set_mode(mode = "classification") ``` --- class: your-turn ## Your turn 1 Run the chunk in your .Rmd and look at the output. Then, copy/paste the code and edit to create: + a decision tree model for classification + that uses the `C5.0` engine. Save it as `tree_mod` and look at the object. What is different about the output? *Hint: you&#39;ll need https://www.tidymodels.org/find/parsnip/* <div class="countdown" id="timer_62f3ecba" style="right:0;bottom:0;" data-warnwhen="0"> <code class="countdown-time"><span class="countdown-digits minutes">03</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code> </div> --- ```r lr_mod ## Logistic Regression Model Specification (classification) ## ## Computational engine: glm tree_mod <- decision_tree() %>% set_engine(engine = "C5.0") %>% set_mode("classification") tree_mod ## Decision Tree Model Specification (classification) ## ## Computational engine: C5.0 ``` --- class: inverse, middle ## Now we&#39;ve built a model. -- ## But, how do we .white[use] a model? -- ## First - what does it mean to use a model? --- class: inverse, middle, center ![](https://media.giphy.com/media/fhAwk4DnqNgw8/giphy.gif) Statistical models learn from the data. Many learn model parameters, which *can* be useful as values for inference and interpretation. --- ## A fitted model .pull-left[ ```r lr_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel) %>% broom::tidy() ## # A tibble: 3 Ã— 5 ## term estimate std.error statistic p.value ## <chr> <dbl> <dbl> <dbl> <dbl> ## 1 (Intercept) 2.70 0.436 6.20 5.64e-10 ## 2 metascore 0.0202 0.00481 4.20 2.66e- 5 ## 3 imdb_rating -0.606 0.0889 -6.82 8.87e-12 ``` ] .pull-right[ <img src="index_files/figure-html/unnamed-chunk-27-1.png" width="80%" style="display: block; margin: auto;" /> ] --- ## "All models are wrong, but some are useful" <img src="index_files/figure-html/unnamed-chunk-29-1.png" width="80%" style="display: block; margin: auto;" /> --- ## "All models are wrong, but some are useful" <img src="index_files/figure-html/unnamed-chunk-31-1.png" width="80%" style="display: block; margin: auto;" /> --- ## Predict new data ```r bechdel_new <- tibble(metascore = c(40, 50, 60), imdb_rating = c(6, 6, 6), test = c("Fail", "Fail", "Pass")) %>% mutate(test = factor(test, levels = c("Fail", "Pass"))) bechdel_new ## # A tibble: 3 Ã— 3 ## metascore imdb_rating test ## <dbl> <dbl> <fct> ## 1 40 6 Fail ## 2 50 6 Fail ## 3 60 6 Pass ``` --- ## Predict old data ```r tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel) %>% predict(new_data = bechdel) %>% mutate(true_bechdel = bechdel$test) %>% accuracy(truth = true_bechdel, estimate = .pred_class) ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.580 ``` --- ## Predict new data .pull-left[ ### out with the old... ```r tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel) %>% predict(new_data = bechdel) %>% mutate(true_bechdel = bechdel$test) %>% accuracy(truth = true_bechdel, estimate = .pred_class) ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.580 ``` ] .pull-right[ ### in with the ğŸ†• ```r tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel) %>% * predict(new_data = bechdel_new) %>% * mutate(true_bechdel = bechdel_new$test) %>% accuracy(truth = true_bechdel, estimate = .pred_class) ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.333 ``` ] --- ## `fit()` Train a model by fitting a model. Returns a parsnip model fit. ```r fit(tree_mod, test ~ metascore + imdb_rating, data = bechdel) ``` --- ## `fit()` Train a model by fitting a model. Returns a parsnip model fit. ```r tree_mod %>% # parsnip model fit(test ~ metascore + imdb_rating, # a formula data = bechdel # dataframe ) ``` --- ## `fit()` Train a model by fitting a model. Returns a parsnip model fit. ```r tree_fit <- tree_mod %>% # parsnip model fit(test ~ metascore + imdb_rating, # a formula data = bechdel # dataframe ) ``` --- class: center <img src="images/predicting/predicting.001.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.003.jpeg" width="80%" style="display: block; margin: auto;" /> --- ## `predict()` Use a fitted model to predict new `y` values from data. Returns a tibble. ```r predict(tree_fit, new_data = bechdel_new) ``` --- ```r tree_fit %>% predict(new_data = bechdel_new) ## # A tibble: 3 Ã— 1 ## .pred_class ## <fct> ## 1 Pass ## 2 Pass ## 3 Pass ``` --- ## Axiom The best way to measure a model&#39;s performance at predicting new data is to .display[predict new data]. --- ## Data splitting <img src="index_files/figure-html/all-split-1.png" width="80%" style="display: block; margin: auto;" /> --- class: inverse, middle # â™»ï¸ Resample models with `rsample` --- ## `rsample` <iframe src="https://tidymodels.github.io/rsample/" width="100%" height="400px" data-external="1"></iframe> --- ## `initial_split()*` "Splits" data randomly into a single testing and a single training set. ```r initial_split(data, prop = 3/4) ``` .footnote[`*` from `rsample`] --- ```r bechdel_split <- initial_split(data = bechdel, strata = test, prop = 3/4) bechdel_split ## <Training/Testing/Total> ## <1045/349/1394> ``` --- ## `training()` and `testing()*` Extract training and testing sets from an `rsplit` ```r training(bechdel_split) testing(bechdel_split) ``` .footnote[`*` from `rsample`] --- ```r bechdel_train <- training(bechdel_split) bechdel_train ## # A tibble: 1,045 Ã— 10 ## year title test budgeâ€¦Â¹ domgrâ€¦Â² intgrâ€¦Â³ rated metasâ€¦â´ ## <dbl> <chr> <fct> <dbl> <dbl> <dbl> <chr> <dbl> ## 1 2013 12 Yeaâ€¦ Fail 2 5.31 15.9 R 97 ## 2 2013 2 Guns Fail 6.1 7.56 13.2 R 55 ## 3 2013 42 Fail 4 9.50 9.50 PG-13 62 ## 4 2013 47 Ronâ€¦ Fail 22.5 3.84 14.6 PG-13 29 ## 5 2013 A Goodâ€¦ Fail 9.2 6.73 30.4 R 28 ## 6 2013 After â€¦ Fail 13 6.05 24.4 PG-13 33 ## 7 2013 Cloudyâ€¦ Fail 7.8 12.0 27.2 PG 59 ## 8 2013 Don Jon Fail 0.55 2.45 2.64 R 66 ## 9 2013 Escapeâ€¦ Fail 7 2.52 10.4 R 49 ## 10 2013 Gangstâ€¦ Fail 6 4.60 10.4 R 40 ## # â€¦ with 1,035 more rows, 2 more variables: ## # imdb_rating <dbl>, genre <chr>, and abbreviated ## # variable names Â¹â€‹budget_2013, Â²â€‹domgross_2013, ## # Â³â€‹intgross_2013, â´â€‹metascore ## # â„¹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names ``` --- class: center <img src="images/predicting/predicting.001.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.003.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.004.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.006.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.007.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.008.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.009.jpeg" width="80%" style="display: block; margin: auto;" /> --- ## Your turn 2 Fill in the blanks. Use `initial_split()`, `training()`, and `testing()` to: 1. Split **bechdel** into training and test sets. Save the rsplit! 2. Extract the training data and fit your classification tree model. 3. Predict the testing data, and save the true `test` values. 4. Measure the accuracy of your model with your test set. Keep `set.seed(100)` at the start of your code. <div class="countdown" id="timer_62f3ebcf" style="right:0;bottom:0;" data-warnwhen="0"> <code class="countdown-time"><span class="countdown-digits minutes">04</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code> </div> --- ```r set.seed(100) # Important! bechdel_split <- initial_split(bechdel, strata = test, prop = 3/4) bechdel_train <- training(bechdel_split) bechdel_test <- testing(bechdel_split) tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel_train) %>% predict(new_data = bechdel_test) %>% mutate(true_bechdel = bechdel_test$test) %>% accuracy(truth = true_bechdel, estimate = .pred_class) ``` --- class: inverse, middle # Goal of Machine Learning ## ğŸ¯ generate accurate predictions --- ## Axiom Better Model = Better Predictions (Lower error rate) --- ## `accuracy()*` Calculates the accuracy based on two columns in a dataframe: The .display[truth]: `\({y}_i\)` The predicted .display[estimate]: `\(\hat{y}_i\)` ```r accuracy(data, truth, estimate) ``` .footnote[`*` from `yardstick`] --- ```r tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel_train) %>% predict(new_data = bechdel_test) %>% mutate(true_bechdel = bechdel_test$test) %>% * accuracy(truth = true_bechdel, estimate = .pred_class) ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.547 ``` --- class: center <img src="images/predicting/predicting.006.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.007.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.008.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.009.jpeg" width="80%" style="display: block; margin: auto;" /> --- ## Your Turn 3 What would happen if you repeated this process? Would you get the same answers? Note your accuracy from above. Then change your seed number and rerun just the last code chunk above. Do you get the same answer? Try it a few times with a few different seeds. <div class="countdown" id="timer_62f3e974" style="right:0;bottom:0;" data-warnwhen="0"> <code class="countdown-time"><span class="countdown-digits minutes">02</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code> </div> --- .pull-left[ ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.553 ``` ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.559 ``` ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.567 ``` ] -- .pull-right[ ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.539 ``` ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.504 ``` ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.550 ``` ] --- ## Quiz Why is the new estimate different? --- ## Data Splitting -- <img src="index_files/figure-html/unnamed-chunk-74-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-75-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-76-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-77-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-78-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-79-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-80-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-81-1.png" width="80%" style="display: block; margin: auto;" /> --- <img src="index_files/figure-html/unnamed-chunk-82-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-83-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-84-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-85-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-86-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-87-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-88-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-89-1.png" width="80%" style="display: block; margin: auto;" /> -- .right[Mean accuracy] --- ## Resampling Let&#39;s resample 10 times then compute the mean of the results... --- ```r acc %>% tibble::enframe(name = "accuracy") ## # A tibble: 10 Ã— 2 ## accuracy value ## <int> <dbl> ## 1 1 0.550 ## 2 2 0.625 ## 3 3 0.599 ## 4 4 0.587 ## 5 5 0.605 ## 6 6 0.593 ## 7 7 0.542 ## 8 8 0.530 ## 9 9 0.564 ## 10 10 0.599 mean(acc) ## [1] 0.5793696 ``` --- ## Guess Which do you think is a better estimate? The best result or the mean of the results? Why? --- ## But also... Fit with .display[training set] Predict with .display[testing set] -- Rinse and repeat? --- ## There has to be a better way... ```r acc <- vector(length = 10, mode = "double") for (i in 1:10) { new_split <- initial_split(bechdel) new_train <- training(new_split) new_test <- testing(new_split) acc[i] <- lr_mod %>% fit(test ~ metascore + imdb_rating, data = new_train) %>% predict(new_test) %>% mutate(truth = new_test$test) %>% accuracy(truth, .pred_class) %>% pull(.estimate) } ``` --- background-image: url(images/diamonds.jpg) background-size: contain background-position: left class: middle, center background-color: #f5f5f5 .pull-right[ ## The .display[testing set] is precious... ## we can only use it once! ] --- background-image: url(images/diamonds.jpg) background-size: contain background-position: left class: middle, center background-color: #f5f5f5 .pull-right[ ## How can we use the training set to compare, evaluate, and tune models? ] --- class: middle <img src="https://www.tidymodels.org/start/resampling/img/resampling.svg" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide2.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide3.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide4.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide5.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide6.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide7.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide8.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide9.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide10.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide11.png" width="80%" style="display: block; margin: auto;" /> --- ## V-fold cross-validation ```r vfold_cv(data, v = 10, ...) ``` --- exclude: true --- ## Guess How many times does an observation/row appear in the assessment set? <img src="index_files/figure-html/vfold-tiles-1.png" width="80%" style="display: block; margin: auto;" /> --- <img src="index_files/figure-html/unnamed-chunk-105-1.png" width="80%" style="display: block; margin: auto;" /> --- ## Quiz If we use 10 folds, which percent of our data will end up in the training set and which percent in the testing set for each fold? -- 90% - training 10% - test --- ## Your Turn 4 Run the code below. What does it return? ```r set.seed(100) bechdel_folds <- vfold_cv(data = bechdel_train, v = 10, strata = test) bechdel_folds ``` <div class="countdown" id="timer_62f3e9e7" style="right:0;bottom:0;" data-warnwhen="0"> <code class="countdown-time"><span class="countdown-digits minutes">01</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code> </div> --- ```r set.seed(100) bechdel_folds <- vfold_cv(data = bechdel_train, v = 10, strata = test) bechdel_folds ## # 10-fold cross-validation using stratification ## # A tibble: 10 Ã— 2 ## splits id ## <list> <chr> ## 1 <split [940/105]> Fold01 ## 2 <split [940/105]> Fold02 ## 3 <split [940/105]> Fold03 ## 4 <split [940/105]> Fold04 ## 5 <split [940/105]> Fold05 ## 6 <split [940/105]> Fold06 ## 7 <split [941/104]> Fold07 ## 8 <split [941/104]> Fold08 ## 9 <split [941/104]> Fold09 ## 10 <split [942/103]> Fold10 ``` --- ## We need a new way to fit ```r split1 <- bechdel_folds %>% pluck("splits", 1) split1_train <- training(split1) split1_test <- testing(split1) tree_mod %>% fit(test ~ ., data = split1_train) %>% predict(split1_test) %>% mutate(truth = split1_test$test) %>% accuracy(truth, .pred_class) # rinse and repeat split2 <- ... ``` --- ## `fit_resamples()` Trains and tests a resampled model. ```r tree_mod %>% fit_resamples( test ~ metascore + imdb_rating, resamples = bechdel_folds ) ``` --- ```r tree_mod %>% fit_resamples( test ~ metascore + imdb_rating, resamples = bechdel_folds ) ## # Resampling results ## # 10-fold cross-validation using stratification ## # A tibble: 10 Ã— 4 ## splits id .metrics .notes ## <list> <chr> <list> <list> ## 1 <split [940/105]> Fold01 <tibble [2 Ã— 4]> <tibble> ## 2 <split [940/105]> Fold02 <tibble [2 Ã— 4]> <tibble> ## 3 <split [940/105]> Fold03 <tibble [2 Ã— 4]> <tibble> ## 4 <split [940/105]> Fold04 <tibble [2 Ã— 4]> <tibble> ## 5 <split [940/105]> Fold05 <tibble [2 Ã— 4]> <tibble> ## 6 <split [940/105]> Fold06 <tibble [2 Ã— 4]> <tibble> ## 7 <split [941/104]> Fold07 <tibble [2 Ã— 4]> <tibble> ## 8 <split [941/104]> Fold08 <tibble [2 Ã— 4]> <tibble> ## 9 <split [941/104]> Fold09 <tibble [2 Ã— 4]> <tibble> ## 10 <split [942/103]> Fold10 <tibble [2 Ã— 4]> <tibble> ``` --- ## `collect_metrics()` Unnest the metrics column from a tidymodels `fit_resamples()` ```r _results %>% collect_metrics(summarize = TRUE) ``` -- .footnote[`TRUE` is actually the default; averages across folds] --- ```r tree_mod %>% fit_resamples( test ~ metascore + imdb_rating, resamples = bechdel_folds ) %>% collect_metrics(summarize = FALSE) ## # A tibble: 20 Ã— 5 ## id .metric .estimator .estimate .config ## <chr> <chr> <chr> <dbl> <chr> ## 1 Fold01 accuracy binary 0.610 Preprocessor1_Model1 ## 2 Fold01 roc_auc binary 0.587 Preprocessor1_Model1 ## 3 Fold02 accuracy binary 0.438 Preprocessor1_Model1 ## 4 Fold02 roc_auc binary 0.434 Preprocessor1_Model1 ## 5 Fold03 accuracy binary 0.6 Preprocessor1_Model1 ## 6 Fold03 roc_auc binary 0.570 Preprocessor1_Model1 ## 7 Fold04 accuracy binary 0.562 Preprocessor1_Model1 ## 8 Fold04 roc_auc binary 0.547 Preprocessor1_Model1 ## 9 Fold05 accuracy binary 0.6 Preprocessor1_Model1 ## 10 Fold05 roc_auc binary 0.572 Preprocessor1_Model1 ## # â€¦ with 10 more rows ## # â„¹ Use `print(n = ...)` to see more rows ``` --- ## 10-fold CV - 10 different analysis/assessment sets - 10 different models (trained on .display[analysis] sets) - 10 different sets of performance statistics (on .display[assessment] sets) --- ## Your Turn 5 Modify the code below to use `fit_resamples` and `bechdel_folds` to cross-validate the classification tree model. What is the ROC AUC that you collect at the end? ```r set.seed(100) tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel_train) %>% predict(new_data = bechdel_test) %>% mutate(true_bechdel = bechdel_test$test) %>% accuracy(truth = true_bechdel, estimate = .pred_class) ``` <div class="countdown" id="timer_62f3ebc2" style="right:0;bottom:0;" data-warnwhen="0"> <code class="countdown-time"><span class="countdown-digits minutes">03</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>'><meta itemprop=wordCount content="3628"><meta name=twitter:card content="summary"><meta name=twitter:title content="CFSS: Computing for the Social Sciences"><meta name=twitter:description content='<!DOCTYPE html> An introduction to machine learning class: center, middle, inverse, title-slide .title[ # An introduction to machine learning ] .author[ ### INFO 5940 Cornell University ] --- class: inverse, middle # What is machine learning? --- class: middle <img src="../../../../../../../../img/amazon-recommendations.png" width="80%" style="display: block; margin: auto;" /> --- class: middle <img src="https://miro.medium.com/max/1400/1*j5aWfH9t1_EZPJC92CJ7oQ.png" width="80%" style="display: block; margin: auto;" /> .footnote[https://medium.com/tmobile-tech/small-data-big-value-f783ceca4fdb] --- class: middle <img src="https://techcrunch.com/wp-content/uploads/2017/12/facebook-facial-recognition-photo-review.png?w=730&amp;crop=1" width="80%" style="display: block; margin: auto;" /> --- class: middle, inverse # What is machine learning? --- class: middle, center <img src="https://imgs.xkcd.com/comics/machine_learning.png" width="40%" style="display: block; margin: auto;" /> --- class: middle <img src="images/intro.002.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: middle <img src="images/intro.003.jpeg" width="80%" style="display: block; margin: auto;" /> --- ## Types of machine learning - Supervised - Unsupervised --- <img src="images/all-of-ml.jpg" width="80%" style="display: block; margin: auto;" /> .footnote[Credit: <https://vas3k.com/blog/machine_learning/>] --- ## Examples of supervised learning - Ad click prediction - Supreme Court decision making - Police misconduct prediction --- ## Two modes -- .pull-left[ ### Classification Is your hospital in a state of crisis care? ] -- .pull-left[ ### Regression What percentage of hospital beds are occupied? ] --- ## Two cultures .pull-left[ ### Statistics - model first - inference emphasis ] .pull-right[ ### Machine Learning - data first - prediction emphasis ] --- name: train-love background-image: url(images/train.jpg) background-size: contain background-color: #f6f6f6 --- template: train-love class: center, top # Statistics --- template: train-love class: bottom > *"Statisticians, like artists, have the bad habit of falling in love with their models."* > > &amp;mdash; George Box --- ## Predictive modeling ```r library(tidymodels) ``` ``` ## â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.0.0 â”€â”€ ``` ``` ## âœ” broom 1.0.0 âœ” recipes 1.0.1 ## âœ” dials 1.0.0 âœ” rsample 1.1.0 ## âœ” dplyr 1.0.9 âœ” tibble 3.1.8 ## âœ” ggplot2 3.3.6 âœ” tidyr 1.2.0 ## âœ” infer 1.0.2 âœ” tune 1.0.0 ## âœ” modeldata 1.0.0 âœ” workflows 1.0.0 ## âœ” parsnip 1.0.0 âœ” workflowsets 1.0.0 ## âœ” purrr 0.3.4 âœ” yardstick 1.0.0 ``` ``` ## â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€ ## âœ– purrr::discard() masks scales::discard() ## âœ– dplyr::filter() masks stats::filter() ## âœ– dplyr::lag() masks stats::lag() ## âœ– recipes::step() masks stats::step() ## â€¢ Search for functions across packages at https://www.tidymodels.org/find/ ``` --- ## Bechdel Test <img src="https://fivethirtyeight.com/wp-content/uploads/2014/04/hickey-bechdel-11.png" width="60%" style="display: block; margin: auto;" /> .footnote[Source:[FiveThirtyEight](https://fivethirtyeight.com/features/the-dollar-and-cents-case-against-hollywoods-exclusion-of-women/)] --- ## Bechdel Test 1. It has to have at least two [named] women in it 1. Who talk to each other 1. About something besides a man -- ```r library(rcis) data("bechdel") glimpse(bechdel) ## Rows: 1,394 ## Columns: 10 ## $ year <dbl> 2013, 2013, 2013, 2013, 2013, 2013, â€¦ ## $ title <chr> "12 Years a Slave", "2 Guns", "42", â€¦ ## $ test <fct> Fail, Fail, Fail, Fail, Fail, Pass, â€¦ ## $ budget_2013 <dbl> 2.00, 6.10, 4.00, 22.50, 9.20, 1.20,â€¦ ## $ domgross_2013 <dbl> 5.310703, 7.561246, 9.502021, 3.8362â€¦ ## $ intgross_2013 <dbl> 15.860703, 13.249301, 9.502021, 14.5â€¦ ## $ rated <chr> "R", "R", "PG-13", "PG-13", "R", "R"â€¦ ## $ metascore <dbl> 97, 55, 62, 29, 28, 55, 48, 33, 90, â€¦ ## $ imdb_rating <dbl> 8.3, 6.8, 7.6, 6.6, 5.4, 7.8, 5.7, 5â€¦ ## $ genre <chr> "Biography", "Action", "Biography", â€¦ ``` --- ## Bechdel test data - N = 1394 - 1 categorical outcome: `test` - 9 predictors --- class: middle <img src="index_files/figure-html/unnamed-chunk-11-1.png" width="80%" style="display: block; margin: auto;" /> --- class: inverse, middle # What is the goal of machine learning? -- ## Build .white[models] that -- ## generate .white[accurate predictions] -- ## for .white[future, yet-to-be-seen data]. -- .footnote[Max Kuhn & Kjell Johnston, http://www.feat.engineering/] --- ## Machine learning We&#39;ll use this goal to drive learning of 3 core `tidymodels` packages: - `parsnip` - `yardstick` - `rsample` --- class: inverse, middle ## ğŸ”¨ Build models with `parsnip` --- class: middle, center, frame ## parsnip <iframe src="https://parsnip.tidymodels.org" width="100%" height="400px" data-external="1"></iframe> --- ## `glm()` ```r glm(test ~ metascore, family = binomial, data = bechdel) ## ## Call: glm(formula = test ~ metascore, family = binomial, data = bechdel) ## ## Coefficients: ## (Intercept) metascore ## 0.052274 -0.004563 ## ## Degrees of Freedom: 1393 Total (i.e. Null); 1392 Residual ## Null Deviance:	1916 ## Residual Deviance: 1914 AIC: 1918 ``` --- class: center <img src="images/predicting/predicting.001.jpeg" width="80%" style="display: block; margin: auto;" /> --- ## To specify a model with `parsnip` 1. Pick a model 1. Set the engine 1. Set the mode (if needed) --- ## To specify a model with `parsnip` ```r logistic_reg() %>% set_engine("glm") %>% set_mode("classification") ## Logistic Regression Model Specification (classification) ## ## Computational engine: glm ``` --- ## To specify a model with `parsnip` ```r decision_tree() %>% set_engine("C5.0") %>% set_mode("classification") ## Decision Tree Model Specification (classification) ## ## Computational engine: C5.0 ``` --- ## To specify a model with `parsnip` ```r nearest_neighbor() %>% set_engine("kknn") %>% set_mode("classification") ## K-Nearest Neighbor Model Specification (classification) ## ## Computational engine: kknn ``` --- ## 1\. Pick a model All available models are listed at <https://www.tidymodels.org/find/parsnip/> <iframe src="https://www.tidymodels.org/find/parsnip/" width="100%" height="400px" data-external="1"></iframe> --- ## `logistic_reg()` Specifies a model that uses logistic regression ```r logistic_reg(penalty = NULL, mixture = NULL) ``` --- ## `logistic_reg()` Specifies a model that uses logistic regression ```r logistic_reg( mode = "classification", # "default" mode, if exists penalty = NULL, # model hyper-parameter mixture = NULL # model hyper-parameter ) ``` --- ## `set_engine()` Adds an engine to power or implement the model. ```r logistic_reg() %>% set_engine(engine = "glm") ``` --- ## `set_mode()` Sets the class of problem the model will solve, which influences which output is collected. Not necessary if mode is set in Step 1. ```r logistic_reg() %>% set_mode(mode = "classification") ``` --- class: your-turn ## Your turn 1 Run the chunk in your .Rmd and look at the output. Then, copy/paste the code and edit to create: + a decision tree model for classification + that uses the `C5.0` engine. Save it as `tree_mod` and look at the object. What is different about the output? *Hint: you&#39;ll need https://www.tidymodels.org/find/parsnip/* <div class="countdown" id="timer_62f3ecba" style="right:0;bottom:0;" data-warnwhen="0"> <code class="countdown-time"><span class="countdown-digits minutes">03</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code> </div> --- ```r lr_mod ## Logistic Regression Model Specification (classification) ## ## Computational engine: glm tree_mod <- decision_tree() %>% set_engine(engine = "C5.0") %>% set_mode("classification") tree_mod ## Decision Tree Model Specification (classification) ## ## Computational engine: C5.0 ``` --- class: inverse, middle ## Now we&#39;ve built a model. -- ## But, how do we .white[use] a model? -- ## First - what does it mean to use a model? --- class: inverse, middle, center ![](https://media.giphy.com/media/fhAwk4DnqNgw8/giphy.gif) Statistical models learn from the data. Many learn model parameters, which *can* be useful as values for inference and interpretation. --- ## A fitted model .pull-left[ ```r lr_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel) %>% broom::tidy() ## # A tibble: 3 Ã— 5 ## term estimate std.error statistic p.value ## <chr> <dbl> <dbl> <dbl> <dbl> ## 1 (Intercept) 2.70 0.436 6.20 5.64e-10 ## 2 metascore 0.0202 0.00481 4.20 2.66e- 5 ## 3 imdb_rating -0.606 0.0889 -6.82 8.87e-12 ``` ] .pull-right[ <img src="index_files/figure-html/unnamed-chunk-27-1.png" width="80%" style="display: block; margin: auto;" /> ] --- ## "All models are wrong, but some are useful" <img src="index_files/figure-html/unnamed-chunk-29-1.png" width="80%" style="display: block; margin: auto;" /> --- ## "All models are wrong, but some are useful" <img src="index_files/figure-html/unnamed-chunk-31-1.png" width="80%" style="display: block; margin: auto;" /> --- ## Predict new data ```r bechdel_new <- tibble(metascore = c(40, 50, 60), imdb_rating = c(6, 6, 6), test = c("Fail", "Fail", "Pass")) %>% mutate(test = factor(test, levels = c("Fail", "Pass"))) bechdel_new ## # A tibble: 3 Ã— 3 ## metascore imdb_rating test ## <dbl> <dbl> <fct> ## 1 40 6 Fail ## 2 50 6 Fail ## 3 60 6 Pass ``` --- ## Predict old data ```r tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel) %>% predict(new_data = bechdel) %>% mutate(true_bechdel = bechdel$test) %>% accuracy(truth = true_bechdel, estimate = .pred_class) ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.580 ``` --- ## Predict new data .pull-left[ ### out with the old... ```r tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel) %>% predict(new_data = bechdel) %>% mutate(true_bechdel = bechdel$test) %>% accuracy(truth = true_bechdel, estimate = .pred_class) ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.580 ``` ] .pull-right[ ### in with the ğŸ†• ```r tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel) %>% * predict(new_data = bechdel_new) %>% * mutate(true_bechdel = bechdel_new$test) %>% accuracy(truth = true_bechdel, estimate = .pred_class) ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.333 ``` ] --- ## `fit()` Train a model by fitting a model. Returns a parsnip model fit. ```r fit(tree_mod, test ~ metascore + imdb_rating, data = bechdel) ``` --- ## `fit()` Train a model by fitting a model. Returns a parsnip model fit. ```r tree_mod %>% # parsnip model fit(test ~ metascore + imdb_rating, # a formula data = bechdel # dataframe ) ``` --- ## `fit()` Train a model by fitting a model. Returns a parsnip model fit. ```r tree_fit <- tree_mod %>% # parsnip model fit(test ~ metascore + imdb_rating, # a formula data = bechdel # dataframe ) ``` --- class: center <img src="images/predicting/predicting.001.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.003.jpeg" width="80%" style="display: block; margin: auto;" /> --- ## `predict()` Use a fitted model to predict new `y` values from data. Returns a tibble. ```r predict(tree_fit, new_data = bechdel_new) ``` --- ```r tree_fit %>% predict(new_data = bechdel_new) ## # A tibble: 3 Ã— 1 ## .pred_class ## <fct> ## 1 Pass ## 2 Pass ## 3 Pass ``` --- ## Axiom The best way to measure a model&#39;s performance at predicting new data is to .display[predict new data]. --- ## Data splitting <img src="index_files/figure-html/all-split-1.png" width="80%" style="display: block; margin: auto;" /> --- class: inverse, middle # â™»ï¸ Resample models with `rsample` --- ## `rsample` <iframe src="https://tidymodels.github.io/rsample/" width="100%" height="400px" data-external="1"></iframe> --- ## `initial_split()*` "Splits" data randomly into a single testing and a single training set. ```r initial_split(data, prop = 3/4) ``` .footnote[`*` from `rsample`] --- ```r bechdel_split <- initial_split(data = bechdel, strata = test, prop = 3/4) bechdel_split ## <Training/Testing/Total> ## <1045/349/1394> ``` --- ## `training()` and `testing()*` Extract training and testing sets from an `rsplit` ```r training(bechdel_split) testing(bechdel_split) ``` .footnote[`*` from `rsample`] --- ```r bechdel_train <- training(bechdel_split) bechdel_train ## # A tibble: 1,045 Ã— 10 ## year title test budgeâ€¦Â¹ domgrâ€¦Â² intgrâ€¦Â³ rated metasâ€¦â´ ## <dbl> <chr> <fct> <dbl> <dbl> <dbl> <chr> <dbl> ## 1 2013 12 Yeaâ€¦ Fail 2 5.31 15.9 R 97 ## 2 2013 2 Guns Fail 6.1 7.56 13.2 R 55 ## 3 2013 42 Fail 4 9.50 9.50 PG-13 62 ## 4 2013 47 Ronâ€¦ Fail 22.5 3.84 14.6 PG-13 29 ## 5 2013 A Goodâ€¦ Fail 9.2 6.73 30.4 R 28 ## 6 2013 After â€¦ Fail 13 6.05 24.4 PG-13 33 ## 7 2013 Cloudyâ€¦ Fail 7.8 12.0 27.2 PG 59 ## 8 2013 Don Jon Fail 0.55 2.45 2.64 R 66 ## 9 2013 Escapeâ€¦ Fail 7 2.52 10.4 R 49 ## 10 2013 Gangstâ€¦ Fail 6 4.60 10.4 R 40 ## # â€¦ with 1,035 more rows, 2 more variables: ## # imdb_rating <dbl>, genre <chr>, and abbreviated ## # variable names Â¹â€‹budget_2013, Â²â€‹domgross_2013, ## # Â³â€‹intgross_2013, â´â€‹metascore ## # â„¹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names ``` --- class: center <img src="images/predicting/predicting.001.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.003.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.004.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.006.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.007.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.008.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.009.jpeg" width="80%" style="display: block; margin: auto;" /> --- ## Your turn 2 Fill in the blanks. Use `initial_split()`, `training()`, and `testing()` to: 1. Split **bechdel** into training and test sets. Save the rsplit! 2. Extract the training data and fit your classification tree model. 3. Predict the testing data, and save the true `test` values. 4. Measure the accuracy of your model with your test set. Keep `set.seed(100)` at the start of your code. <div class="countdown" id="timer_62f3ebcf" style="right:0;bottom:0;" data-warnwhen="0"> <code class="countdown-time"><span class="countdown-digits minutes">04</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code> </div> --- ```r set.seed(100) # Important! bechdel_split <- initial_split(bechdel, strata = test, prop = 3/4) bechdel_train <- training(bechdel_split) bechdel_test <- testing(bechdel_split) tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel_train) %>% predict(new_data = bechdel_test) %>% mutate(true_bechdel = bechdel_test$test) %>% accuracy(truth = true_bechdel, estimate = .pred_class) ``` --- class: inverse, middle # Goal of Machine Learning ## ğŸ¯ generate accurate predictions --- ## Axiom Better Model = Better Predictions (Lower error rate) --- ## `accuracy()*` Calculates the accuracy based on two columns in a dataframe: The .display[truth]: `\({y}_i\)` The predicted .display[estimate]: `\(\hat{y}_i\)` ```r accuracy(data, truth, estimate) ``` .footnote[`*` from `yardstick`] --- ```r tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel_train) %>% predict(new_data = bechdel_test) %>% mutate(true_bechdel = bechdel_test$test) %>% * accuracy(truth = true_bechdel, estimate = .pred_class) ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.547 ``` --- class: center <img src="images/predicting/predicting.006.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.007.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.008.jpeg" width="80%" style="display: block; margin: auto;" /> --- class: center <img src="images/predicting/predicting.009.jpeg" width="80%" style="display: block; margin: auto;" /> --- ## Your Turn 3 What would happen if you repeated this process? Would you get the same answers? Note your accuracy from above. Then change your seed number and rerun just the last code chunk above. Do you get the same answer? Try it a few times with a few different seeds. <div class="countdown" id="timer_62f3e974" style="right:0;bottom:0;" data-warnwhen="0"> <code class="countdown-time"><span class="countdown-digits minutes">02</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code> </div> --- .pull-left[ ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.553 ``` ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.559 ``` ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.567 ``` ] -- .pull-right[ ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.539 ``` ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.504 ``` ``` ## # A tibble: 1 Ã— 3 ## .metric .estimator .estimate ## <chr> <chr> <dbl> ## 1 accuracy binary 0.550 ``` ] --- ## Quiz Why is the new estimate different? --- ## Data Splitting -- <img src="index_files/figure-html/unnamed-chunk-74-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-75-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-76-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-77-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-78-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-79-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-80-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-81-1.png" width="80%" style="display: block; margin: auto;" /> --- <img src="index_files/figure-html/unnamed-chunk-82-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-83-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-84-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-85-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-86-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-87-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-88-1.png" width="80%" style="display: block; margin: auto;" /> -- <img src="index_files/figure-html/unnamed-chunk-89-1.png" width="80%" style="display: block; margin: auto;" /> -- .right[Mean accuracy] --- ## Resampling Let&#39;s resample 10 times then compute the mean of the results... --- ```r acc %>% tibble::enframe(name = "accuracy") ## # A tibble: 10 Ã— 2 ## accuracy value ## <int> <dbl> ## 1 1 0.550 ## 2 2 0.625 ## 3 3 0.599 ## 4 4 0.587 ## 5 5 0.605 ## 6 6 0.593 ## 7 7 0.542 ## 8 8 0.530 ## 9 9 0.564 ## 10 10 0.599 mean(acc) ## [1] 0.5793696 ``` --- ## Guess Which do you think is a better estimate? The best result or the mean of the results? Why? --- ## But also... Fit with .display[training set] Predict with .display[testing set] -- Rinse and repeat? --- ## There has to be a better way... ```r acc <- vector(length = 10, mode = "double") for (i in 1:10) { new_split <- initial_split(bechdel) new_train <- training(new_split) new_test <- testing(new_split) acc[i] <- lr_mod %>% fit(test ~ metascore + imdb_rating, data = new_train) %>% predict(new_test) %>% mutate(truth = new_test$test) %>% accuracy(truth, .pred_class) %>% pull(.estimate) } ``` --- background-image: url(images/diamonds.jpg) background-size: contain background-position: left class: middle, center background-color: #f5f5f5 .pull-right[ ## The .display[testing set] is precious... ## we can only use it once! ] --- background-image: url(images/diamonds.jpg) background-size: contain background-position: left class: middle, center background-color: #f5f5f5 .pull-right[ ## How can we use the training set to compare, evaluate, and tune models? ] --- class: middle <img src="https://www.tidymodels.org/start/resampling/img/resampling.svg" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide2.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide3.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide4.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide5.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide6.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide7.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide8.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide9.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide10.png" width="80%" style="display: block; margin: auto;" /> --- ## Cross-validation <img src="images/cross-validation/Slide11.png" width="80%" style="display: block; margin: auto;" /> --- ## V-fold cross-validation ```r vfold_cv(data, v = 10, ...) ``` --- exclude: true --- ## Guess How many times does an observation/row appear in the assessment set? <img src="index_files/figure-html/vfold-tiles-1.png" width="80%" style="display: block; margin: auto;" /> --- <img src="index_files/figure-html/unnamed-chunk-105-1.png" width="80%" style="display: block; margin: auto;" /> --- ## Quiz If we use 10 folds, which percent of our data will end up in the training set and which percent in the testing set for each fold? -- 90% - training 10% - test --- ## Your Turn 4 Run the code below. What does it return? ```r set.seed(100) bechdel_folds <- vfold_cv(data = bechdel_train, v = 10, strata = test) bechdel_folds ``` <div class="countdown" id="timer_62f3e9e7" style="right:0;bottom:0;" data-warnwhen="0"> <code class="countdown-time"><span class="countdown-digits minutes">01</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code> </div> --- ```r set.seed(100) bechdel_folds <- vfold_cv(data = bechdel_train, v = 10, strata = test) bechdel_folds ## # 10-fold cross-validation using stratification ## # A tibble: 10 Ã— 2 ## splits id ## <list> <chr> ## 1 <split [940/105]> Fold01 ## 2 <split [940/105]> Fold02 ## 3 <split [940/105]> Fold03 ## 4 <split [940/105]> Fold04 ## 5 <split [940/105]> Fold05 ## 6 <split [940/105]> Fold06 ## 7 <split [941/104]> Fold07 ## 8 <split [941/104]> Fold08 ## 9 <split [941/104]> Fold09 ## 10 <split [942/103]> Fold10 ``` --- ## We need a new way to fit ```r split1 <- bechdel_folds %>% pluck("splits", 1) split1_train <- training(split1) split1_test <- testing(split1) tree_mod %>% fit(test ~ ., data = split1_train) %>% predict(split1_test) %>% mutate(truth = split1_test$test) %>% accuracy(truth, .pred_class) # rinse and repeat split2 <- ... ``` --- ## `fit_resamples()` Trains and tests a resampled model. ```r tree_mod %>% fit_resamples( test ~ metascore + imdb_rating, resamples = bechdel_folds ) ``` --- ```r tree_mod %>% fit_resamples( test ~ metascore + imdb_rating, resamples = bechdel_folds ) ## # Resampling results ## # 10-fold cross-validation using stratification ## # A tibble: 10 Ã— 4 ## splits id .metrics .notes ## <list> <chr> <list> <list> ## 1 <split [940/105]> Fold01 <tibble [2 Ã— 4]> <tibble> ## 2 <split [940/105]> Fold02 <tibble [2 Ã— 4]> <tibble> ## 3 <split [940/105]> Fold03 <tibble [2 Ã— 4]> <tibble> ## 4 <split [940/105]> Fold04 <tibble [2 Ã— 4]> <tibble> ## 5 <split [940/105]> Fold05 <tibble [2 Ã— 4]> <tibble> ## 6 <split [940/105]> Fold06 <tibble [2 Ã— 4]> <tibble> ## 7 <split [941/104]> Fold07 <tibble [2 Ã— 4]> <tibble> ## 8 <split [941/104]> Fold08 <tibble [2 Ã— 4]> <tibble> ## 9 <split [941/104]> Fold09 <tibble [2 Ã— 4]> <tibble> ## 10 <split [942/103]> Fold10 <tibble [2 Ã— 4]> <tibble> ``` --- ## `collect_metrics()` Unnest the metrics column from a tidymodels `fit_resamples()` ```r _results %>% collect_metrics(summarize = TRUE) ``` -- .footnote[`TRUE` is actually the default; averages across folds] --- ```r tree_mod %>% fit_resamples( test ~ metascore + imdb_rating, resamples = bechdel_folds ) %>% collect_metrics(summarize = FALSE) ## # A tibble: 20 Ã— 5 ## id .metric .estimator .estimate .config ## <chr> <chr> <chr> <dbl> <chr> ## 1 Fold01 accuracy binary 0.610 Preprocessor1_Model1 ## 2 Fold01 roc_auc binary 0.587 Preprocessor1_Model1 ## 3 Fold02 accuracy binary 0.438 Preprocessor1_Model1 ## 4 Fold02 roc_auc binary 0.434 Preprocessor1_Model1 ## 5 Fold03 accuracy binary 0.6 Preprocessor1_Model1 ## 6 Fold03 roc_auc binary 0.570 Preprocessor1_Model1 ## 7 Fold04 accuracy binary 0.562 Preprocessor1_Model1 ## 8 Fold04 roc_auc binary 0.547 Preprocessor1_Model1 ## 9 Fold05 accuracy binary 0.6 Preprocessor1_Model1 ## 10 Fold05 roc_auc binary 0.572 Preprocessor1_Model1 ## # â€¦ with 10 more rows ## # â„¹ Use `print(n = ...)` to see more rows ``` --- ## 10-fold CV - 10 different analysis/assessment sets - 10 different models (trained on .display[analysis] sets) - 10 different sets of performance statistics (on .display[assessment] sets) --- ## Your Turn 5 Modify the code below to use `fit_resamples` and `bechdel_folds` to cross-validate the classification tree model. What is the ROC AUC that you collect at the end? ```r set.seed(100) tree_mod %>% fit(test ~ metascore + imdb_rating, data = bechdel_train) %>% predict(new_data = bechdel_test) %>% mutate(true_bechdel = bechdel_test$test) %>% accuracy(truth = true_bechdel, estimate = .pred_class) ``` <div class="countdown" id="timer_62f3ebc2" style="right:0;bottom:0;" data-warnwhen="0"> <code class="countdown-time"><span class="countdown-digits minutes">03</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>'><link rel=preload href=/scss/main.min.a8f7f85c6779c2416b7cb99e519b85876b68cc4b701a140b92032a5dad60e161.css as=style integrity="sha256-qPf4XGd5wkFrfLmeUZuFh2tozEtwGhQLkgMqXa1g4WE=" crossorigin=anonymous><link href=/scss/main.min.a8f7f85c6779c2416b7cb99e519b85876b68cc4b701a140b92032a5dad60e161.css rel=stylesheet integrity="sha256-qPf4XGd5wkFrfLmeUZuFh2tozEtwGhQLkgMqXa1g4WE=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script></head><body class=td-page><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"></span><span class=navbar-brand__name>CFSS: Computing for the Social Sciences</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class=nav-link href=/><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/syllabus/><span>Syllabus</span></a></li><li class=nav-item><a class=nav-link href=/schedule/><span>Schedule</span></a></li><li class=nav-item><a class=nav-link href=/assignments/><span>Assignments</span></a></li></ul></div><div class="d-none d-lg-block"></div></div></nav></header><div class="container-fluid td-default td-outer"><main role=main class=td-main><!doctype html><html lang xml:lang><head><title>An introduction to machine learning</title><meta charset=utf-8><meta name=author content="INFO 5940   Cornell University"><script src=index_files/header-attrs/header-attrs.js></script><link href=index_files/panelset/panelset.css rel=stylesheet><script src=index_files/panelset/panelset.js></script><link href=index_files/countdown/countdown.css rel=stylesheet><script src=index_files/countdown/countdown.js></script><link rel=stylesheet href=xaringan-themer.css type=text/css></head><body><textarea id=source>
class: center, middle, inverse, title-slide

.title[
# An introduction to machine learning
]
.author[
### INFO 5940 <br /> Cornell University
]

---




class: inverse, middle

# What is machine learning?

---

class: middle

&lt;img src="../../../../../../../../img/amazon-recommendations.png" width="80%" style="display: block; margin: auto;" /&gt;

---

class: middle

&lt;img src="https://miro.medium.com/max/1400/1*j5aWfH9t1_EZPJC92CJ7oQ.png" width="80%" style="display: block; margin: auto;" /&gt;

.footnote[https://medium.com/tmobile-tech/small-data-big-value-f783ceca4fdb]

---

class: middle

&lt;img src="https://techcrunch.com/wp-content/uploads/2017/12/facebook-facial-recognition-photo-review.png?w=730&amp;crop=1" width="80%" style="display: block; margin: auto;" /&gt;

---

class: middle, inverse

# What is machine learning?

---

class: middle, center

&lt;img src="https://imgs.xkcd.com/comics/machine_learning.png" width="40%" style="display: block; margin: auto;" /&gt;

---

class: middle

&lt;img src="images/intro.002.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

---

class: middle

&lt;img src="images/intro.003.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

---

## Types of machine learning

- Supervised
- Unsupervised

---

&lt;img src="images/all-of-ml.jpg" width="80%" style="display: block; margin: auto;" /&gt;

.footnote[Credit: &lt;https://vas3k.com/blog/machine_learning/&gt;]

---

## Examples of supervised learning

- Ad click prediction
- Supreme Court decision making
- Police misconduct prediction

---

## Two modes

--

.pull-left[

### Classification

Is your hospital in a state of crisis care?

]

--

.pull-left[

### Regression

What percentage of hospital beds are occupied?

]

---

## Two cultures

.pull-left[

### Statistics

- model first
- inference emphasis

]

.pull-right[


### Machine Learning

- data first
- prediction emphasis

]

---
name: train-love
background-image: url(images/train.jpg)
background-size: contain
background-color: #f6f6f6

---
template: train-love
class: center, top

# Statistics

---

template: train-love
class: bottom


&gt; *"Statisticians, like artists, have the bad habit of falling in love with their models."*
&gt;
&gt; &amp;mdash; George Box

---

## Predictive modeling


```r
library(tidymodels)
```

```
## â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.0.0 â”€â”€
```

```
## âœ” broom        1.0.0     âœ” recipes      1.0.1
## âœ” dials        1.0.0     âœ” rsample      1.1.0
## âœ” dplyr        1.0.9     âœ” tibble       3.1.8
## âœ” ggplot2      3.3.6     âœ” tidyr        1.2.0
## âœ” infer        1.0.2     âœ” tune         1.0.0
## âœ” modeldata    1.0.0     âœ” workflows    1.0.0
## âœ” parsnip      1.0.0     âœ” workflowsets 1.0.0
## âœ” purrr        0.3.4     âœ” yardstick    1.0.0
```

```
## â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€
## âœ– purrr::discard() masks scales::discard()
## âœ– dplyr::filter()  masks stats::filter()
## âœ– dplyr::lag()     masks stats::lag()
## âœ– recipes::step()  masks stats::step()
## â€¢ Search for functions across packages at https://www.tidymodels.org/find/
```





---

## Bechdel Test

&lt;img src="https://fivethirtyeight.com/wp-content/uploads/2014/04/hickey-bechdel-11.png" width="60%" style="display: block; margin: auto;" /&gt;

.footnote[Source:[FiveThirtyEight](https://fivethirtyeight.com/features/the-dollar-and-cents-case-against-hollywoods-exclusion-of-women/)]

---

## Bechdel Test

1. It has to have at least two [named] women in it
1. Who talk to each other
1. About something besides a man

--


```r
library(rcis)
data("bechdel")
glimpse(bechdel)
## Rows: 1,394
## Columns: 10
## $ year          &lt;dbl&gt; 2013, 2013, 2013, 2013, 2013, 2013, â€¦
## $ title         &lt;chr&gt; "12 Years a Slave", "2 Guns", "42", â€¦
## $ test          &lt;fct&gt; Fail, Fail, Fail, Fail, Fail, Pass, â€¦
## $ budget_2013   &lt;dbl&gt; 2.00, 6.10, 4.00, 22.50, 9.20, 1.20,â€¦
## $ domgross_2013 &lt;dbl&gt; 5.310703, 7.561246, 9.502021, 3.8362â€¦
## $ intgross_2013 &lt;dbl&gt; 15.860703, 13.249301, 9.502021, 14.5â€¦
## $ rated         &lt;chr&gt; "R", "R", "PG-13", "PG-13", "R", "R"â€¦
## $ metascore     &lt;dbl&gt; 97, 55, 62, 29, 28, 55, 48, 33, 90, â€¦
## $ imdb_rating   &lt;dbl&gt; 8.3, 6.8, 7.6, 6.6, 5.4, 7.8, 5.7, 5â€¦
## $ genre         &lt;chr&gt; "Biography", "Action", "Biography", â€¦
```

---

## Bechdel test data

- N = 1394
- 1 categorical outcome: `test`
- 9 predictors

---

class: middle

&lt;img src="index_files/figure-html/unnamed-chunk-11-1.png" width="80%" style="display: block; margin: auto;" /&gt;


---

class: inverse, middle

# What is the goal of machine learning?

--

## Build .white[models] that

--


## generate .white[accurate predictions]

--


## for .white[future, yet-to-be-seen data].

--

.footnote[Max Kuhn &amp; Kjell Johnston, http://www.feat.engineering/]

---

## Machine learning

We'll use this goal to drive learning of 3 core `tidymodels` packages:

- `parsnip`
- `yardstick`
- `rsample`

---

class: inverse, middle

## ğŸ”¨ Build models with `parsnip`

---

class: middle, center, frame

## parsnip

&lt;iframe src="https://parsnip.tidymodels.org" width="100%" height="400px" data-external="1"&gt;&lt;/iframe&gt;

---

## `glm()`



```r
glm(test ~ metascore, family = binomial, data = bechdel)
## 
## Call:  glm(formula = test ~ metascore, family = binomial, data = bechdel)
## 
## Coefficients:
## (Intercept)    metascore  
##    0.052274    -0.004563  
## 
## Degrees of Freedom: 1393 Total (i.e. Null);  1392 Residual
## Null Deviance:	    1916 
## Residual Deviance: 1914 	AIC: 1918
```

---

class: center

&lt;img src="images/predicting/predicting.001.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

---

## To specify a model with `parsnip`

1. Pick a model
1. Set the engine
1. Set the mode (if needed)

---

## To specify a model with `parsnip`


```r
logistic_reg() %&gt;%
  set_engine("glm") %&gt;%
  set_mode("classification")
## Logistic Regression Model Specification (classification)
## 
## Computational engine: glm
```

---

## To specify a model with `parsnip`


```r
decision_tree() %&gt;%
  set_engine("C5.0") %&gt;%
  set_mode("classification")
## Decision Tree Model Specification (classification)
## 
## Computational engine: C5.0
```

---

## To specify a model with `parsnip`


```r
nearest_neighbor() %&gt;%              
  set_engine("kknn") %&gt;%             
  set_mode("classification")        
## K-Nearest Neighbor Model Specification (classification)
## 
## Computational engine: kknn
```

---

## 1\. Pick a model

All available models are listed at

&lt;https://www.tidymodels.org/find/parsnip/&gt;

&lt;iframe src="https://www.tidymodels.org/find/parsnip/" width="100%" height="400px" data-external="1"&gt;&lt;/iframe&gt;

---

## `logistic_reg()`

Specifies a model that uses logistic regression


```r
logistic_reg(penalty = NULL, mixture = NULL)
```

---

## `logistic_reg()`

Specifies a model that uses logistic regression


```r
logistic_reg(
  mode = "classification", # "default" mode, if exists
  penalty = NULL,          # model hyper-parameter
  mixture = NULL           # model hyper-parameter
  )
```

---

## `set_engine()`

Adds an engine to power or implement the model.


```r
logistic_reg() %&gt;% set_engine(engine = "glm")
```

---

## `set_mode()`

Sets the class of problem the model will solve, which influences which output is collected. Not necessary if mode is set in Step 1.


```r
logistic_reg() %&gt;% set_mode(mode = "classification")
```

---
class: your-turn

## Your turn 1

Run the chunk in your .Rmd and look at the output. Then, copy/paste the code and edit to create:

+ a decision tree model for classification 

+ that uses the `C5.0` engine. 

Save it as `tree_mod` and look at the object. What is different about the output?

*Hint: you'll need https://www.tidymodels.org/find/parsnip/*

<div class="countdown" id="timer_62f3ecba" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">03</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---




```r
lr_mod 
## Logistic Regression Model Specification (classification)
## 
## Computational engine: glm

tree_mod &lt;- decision_tree() %&gt;% 
  set_engine(engine = "C5.0") %&gt;% 
  set_mode("classification")
tree_mod 
## Decision Tree Model Specification (classification)
## 
## Computational engine: C5.0
```

---
class: inverse, middle

## Now we've built a model.

--

## But, how do we .white[use] a model?

--

## First - what does it mean to use a model?

---
class: inverse, middle, center

![](https://media.giphy.com/media/fhAwk4DnqNgw8/giphy.gif)

Statistical models learn from the data. 

Many learn model parameters, which *can* be useful as values for inference and interpretation.

---

## A fitted model

.pull-left[


```r
lr_mod %&gt;% 
  fit(test ~ metascore + imdb_rating, 
      data = bechdel) %&gt;% 
  broom::tidy()
## # A tibble: 3 Ã— 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)   2.70     0.436        6.20 5.64e-10
## 2 metascore     0.0202   0.00481      4.20 2.66e- 5
## 3 imdb_rating  -0.606    0.0889      -6.82 8.87e-12
```
]

.pull-right[

&lt;img src="index_files/figure-html/unnamed-chunk-27-1.png" width="80%" style="display: block; margin: auto;" /&gt;
]

---

## "All models are wrong, but some are useful"



&lt;img src="index_files/figure-html/unnamed-chunk-29-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

## "All models are wrong, but some are useful"



&lt;img src="index_files/figure-html/unnamed-chunk-31-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

## Predict new data


```r
bechdel_new &lt;- tibble(metascore = c(40, 50, 60), 
                      imdb_rating = c(6, 6, 6),
                      test = c("Fail", "Fail", "Pass")) %&gt;% 
  mutate(test = factor(test, levels = c("Fail", "Pass")))
bechdel_new
## # A tibble: 3 Ã— 3
##   metascore imdb_rating test 
##       &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;
## 1        40           6 Fail 
## 2        50           6 Fail 
## 3        60           6 Pass
```

---

## Predict old data


```r
tree_mod %&gt;% 
  fit(test ~ metascore + imdb_rating, 
      data = bechdel) %&gt;% 
  predict(new_data = bechdel) %&gt;% 
  mutate(true_bechdel = bechdel$test) %&gt;% 
  accuracy(truth = true_bechdel, 
           estimate = .pred_class)
## # A tibble: 1 Ã— 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.580
```

---

## Predict new data

.pull-left[
### out with the old...

```r
tree_mod %&gt;% 
  fit(test ~ metascore + imdb_rating, 
      data = bechdel) %&gt;% 
  predict(new_data = bechdel) %&gt;% 
  mutate(true_bechdel = bechdel$test) %&gt;% 
  accuracy(truth = true_bechdel, 
           estimate = .pred_class)
## # A tibble: 1 Ã— 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.580
```

]

.pull-right[

### in with the ğŸ†•


```r
tree_mod %&gt;% 
  fit(test ~ metascore + imdb_rating, 
      data = bechdel) %&gt;% 
* predict(new_data = bechdel_new) %&gt;%
* mutate(true_bechdel = bechdel_new$test) %&gt;%
  accuracy(truth = true_bechdel, 
           estimate = .pred_class)
## # A tibble: 1 Ã— 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.333
```

]

---

## `fit()`

Train a model by fitting a model. Returns a parsnip model fit.


```r
fit(tree_mod, test ~ metascore + imdb_rating, data = bechdel)
```

---

## `fit()`

Train a model by fitting a model. Returns a parsnip model fit.


```r
tree_mod %&gt;%                               # parsnip model
  fit(test ~ metascore + imdb_rating, # a formula
      data = bechdel                       # dataframe
  )
```

---

## `fit()`

Train a model by fitting a model. Returns a parsnip model fit.


```r
tree_fit &lt;- tree_mod %&gt;%                   # parsnip model
  fit(test ~ metascore + imdb_rating, # a formula
      data = bechdel                       # dataframe
  )
```

---

class: center

&lt;img src="images/predicting/predicting.001.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

---

class: center

&lt;img src="images/predicting/predicting.003.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

---

## `predict()`

Use a fitted model to predict new `y` values from data. Returns a tibble.


```r
predict(tree_fit, new_data = bechdel_new) 
```

---


```r
tree_fit %&gt;% 
  predict(new_data = bechdel_new)
## # A tibble: 3 Ã— 1
##   .pred_class
##   &lt;fct&gt;      
## 1 Pass       
## 2 Pass       
## 3 Pass
```

---

## Axiom

The best way to measure a model's performance at predicting new data is to .display[predict new data].

---

## Data splitting

&lt;img src="index_files/figure-html/all-split-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

class: inverse, middle

# â™»ï¸ Resample models with `rsample`

---

## `rsample`

&lt;iframe src="https://tidymodels.github.io/rsample/" width="100%" height="400px" data-external="1"&gt;&lt;/iframe&gt;

---

## `initial_split()*`

"Splits" data randomly into a single testing and a single training set.


```r
initial_split(data, prop = 3/4)
```

.footnote[`*` from `rsample`]

---


```r
bechdel_split &lt;- initial_split(data = bechdel, strata = test,  prop = 3/4)
bechdel_split
## &lt;Training/Testing/Total&gt;
## &lt;1045/349/1394&gt;
```

---

## `training()` and `testing()*`

Extract training and testing sets from an `rsplit`


```r
training(bechdel_split)
testing(bechdel_split)
```

.footnote[`*` from `rsample`]

---


```r
bechdel_train &lt;- training(bechdel_split) 
bechdel_train
## # A tibble: 1,045 Ã— 10
##     year title   test  budgeâ€¦Â¹ domgrâ€¦Â² intgrâ€¦Â³ rated metasâ€¦â´
##    &lt;dbl&gt; &lt;chr&gt;   &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;
##  1  2013 12 Yeaâ€¦ Fail     2       5.31   15.9  R          97
##  2  2013 2 Guns  Fail     6.1     7.56   13.2  R          55
##  3  2013 42      Fail     4       9.50    9.50 PG-13      62
##  4  2013 47 Ronâ€¦ Fail    22.5     3.84   14.6  PG-13      29
##  5  2013 A Goodâ€¦ Fail     9.2     6.73   30.4  R          28
##  6  2013 After â€¦ Fail    13       6.05   24.4  PG-13      33
##  7  2013 Cloudyâ€¦ Fail     7.8    12.0    27.2  PG         59
##  8  2013 Don Jon Fail     0.55    2.45    2.64 R          66
##  9  2013 Escapeâ€¦ Fail     7       2.52   10.4  R          49
## 10  2013 Gangstâ€¦ Fail     6       4.60   10.4  R          40
## # â€¦ with 1,035 more rows, 2 more variables:
## #   imdb_rating &lt;dbl&gt;, genre &lt;chr&gt;, and abbreviated
## #   variable names Â¹â€‹budget_2013, Â²â€‹domgross_2013,
## #   Â³â€‹intgross_2013, â´â€‹metascore
## # â„¹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names
```

---

class: center

&lt;img src="images/predicting/predicting.001.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

---

class: center

&lt;img src="images/predicting/predicting.003.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

---

class: center

&lt;img src="images/predicting/predicting.004.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

---

class: center

&lt;img src="images/predicting/predicting.006.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

---

class: center

&lt;img src="images/predicting/predicting.007.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

---

class: center

&lt;img src="images/predicting/predicting.008.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

---

class: center

&lt;img src="images/predicting/predicting.009.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

---

## Your turn 2

Fill in the blanks. 

Use `initial_split()`, `training()`, and `testing()` to:

1. Split **bechdel** into training and test sets. Save the rsplit!

2. Extract the training data and fit your classification tree model.

3. Predict the testing data, and save the true `test` values.

4. Measure the accuracy of your model with your test set.  

Keep `set.seed(100)` at the start of your code.

<div class="countdown" id="timer_62f3ebcf" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">04</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---


```r
set.seed(100) # Important!

bechdel_split  &lt;- initial_split(bechdel, strata = test, prop = 3/4)
bechdel_train  &lt;- training(bechdel_split)
bechdel_test   &lt;- testing(bechdel_split)

tree_mod %&gt;% 
  fit(test ~ metascore + imdb_rating, 
      data = bechdel_train) %&gt;% 
  predict(new_data = bechdel_test) %&gt;% 
  mutate(true_bechdel = bechdel_test$test) %&gt;% 
  accuracy(truth = true_bechdel, estimate = .pred_class)
```

---

class: inverse, middle

# Goal of Machine Learning

## ğŸ¯ generate accurate predictions

---

## Axiom

Better Model = Better Predictions (Lower error rate)

---

## `accuracy()*`

Calculates the accuracy based on two columns in a dataframe: 

The .display[truth]: `\({y}_i\)` 

The predicted .display[estimate]: `\(\hat{y}_i\)` 


```r
accuracy(data, truth, estimate)
```

.footnote[`*` from `yardstick`]

---


```r
tree_mod %&gt;% 
  fit(test ~ metascore + imdb_rating, 
      data = bechdel_train) %&gt;% 
  predict(new_data = bechdel_test) %&gt;% 
  mutate(true_bechdel = bechdel_test$test) %&gt;% 
* accuracy(truth = true_bechdel, estimate = .pred_class)
## # A tibble: 1 Ã— 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.547
```

---

class: center

&lt;img src="images/predicting/predicting.006.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

---

class: center

&lt;img src="images/predicting/predicting.007.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

---

class: center

&lt;img src="images/predicting/predicting.008.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

---

class: center

&lt;img src="images/predicting/predicting.009.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

---

## Your Turn 3

What would happen if you repeated this process? Would you get the same answers?

Note your accuracy from above. Then change your seed number and rerun just the last code chunk above. Do you get the same answer? 

Try it a few times with a few different seeds.

<div class="countdown" id="timer_62f3e974" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">02</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---

.pull-left[


```
## # A tibble: 1 Ã— 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.553
```





```
## # A tibble: 1 Ã— 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.559
```




```
## # A tibble: 1 Ã— 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.567
```

]

--

.pull-right[




```
## # A tibble: 1 Ã— 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.539
```




```
## # A tibble: 1 Ã— 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.504
```




```
## # A tibble: 1 Ã— 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.550
```

]

---

## Quiz

Why is the new estimate different?



---

## Data Splitting

--

&lt;img src="index_files/figure-html/unnamed-chunk-74-1.png" width="80%" style="display: block; margin: auto;" /&gt;

--

&lt;img src="index_files/figure-html/unnamed-chunk-75-1.png" width="80%" style="display: block; margin: auto;" /&gt;

--

&lt;img src="index_files/figure-html/unnamed-chunk-76-1.png" width="80%" style="display: block; margin: auto;" /&gt;

--

&lt;img src="index_files/figure-html/unnamed-chunk-77-1.png" width="80%" style="display: block; margin: auto;" /&gt;

--

&lt;img src="index_files/figure-html/unnamed-chunk-78-1.png" width="80%" style="display: block; margin: auto;" /&gt;

--

&lt;img src="index_files/figure-html/unnamed-chunk-79-1.png" width="80%" style="display: block; margin: auto;" /&gt;

--

&lt;img src="index_files/figure-html/unnamed-chunk-80-1.png" width="80%" style="display: block; margin: auto;" /&gt;

--

&lt;img src="index_files/figure-html/unnamed-chunk-81-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="index_files/figure-html/unnamed-chunk-82-1.png" width="80%" style="display: block; margin: auto;" /&gt;

--

&lt;img src="index_files/figure-html/unnamed-chunk-83-1.png" width="80%" style="display: block; margin: auto;" /&gt;

--

&lt;img src="index_files/figure-html/unnamed-chunk-84-1.png" width="80%" style="display: block; margin: auto;" /&gt;

--

&lt;img src="index_files/figure-html/unnamed-chunk-85-1.png" width="80%" style="display: block; margin: auto;" /&gt;

--

&lt;img src="index_files/figure-html/unnamed-chunk-86-1.png" width="80%" style="display: block; margin: auto;" /&gt;

--

&lt;img src="index_files/figure-html/unnamed-chunk-87-1.png" width="80%" style="display: block; margin: auto;" /&gt;

--

&lt;img src="index_files/figure-html/unnamed-chunk-88-1.png" width="80%" style="display: block; margin: auto;" /&gt;

--

&lt;img src="index_files/figure-html/unnamed-chunk-89-1.png" width="80%" style="display: block; margin: auto;" /&gt;

--

.right[Mean accuracy]

---

## Resampling

Let's resample 10 times 

then compute the mean of the results...

---







```r
acc %&gt;% tibble::enframe(name = "accuracy")
## # A tibble: 10 Ã— 2
##    accuracy value
##       &lt;int&gt; &lt;dbl&gt;
##  1        1 0.550
##  2        2 0.625
##  3        3 0.599
##  4        4 0.587
##  5        5 0.605
##  6        6 0.593
##  7        7 0.542
##  8        8 0.530
##  9        9 0.564
## 10       10 0.599
mean(acc)
## [1] 0.5793696
```

---

## Guess

Which do you think is a better estimate?

The best result or the mean of the results? Why? 

---

## But also...

Fit with .display[training set]

Predict with .display[testing set]

--

Rinse and repeat?

---

## There has to be a better way...


```r
acc &lt;- vector(length = 10, mode = "double")
for (i in 1:10) {
  new_split &lt;- initial_split(bechdel)
  new_train &lt;- training(new_split)
  new_test  &lt;- testing(new_split)
  acc[i] &lt;-
    lr_mod %&gt;% 
      fit(test ~ metascore + imdb_rating, data = new_train) %&gt;% 
      predict(new_test) %&gt;% 
      mutate(truth = new_test$test) %&gt;% 
      accuracy(truth, .pred_class) %&gt;% 
      pull(.estimate)
}
```

---
background-image: url(images/diamonds.jpg)
background-size: contain
background-position: left
class: middle, center
background-color: #f5f5f5

.pull-right[
## The .display[testing set] is precious...

## we can only use it once!

]

---
background-image: url(images/diamonds.jpg)
background-size: contain
background-position: left
class: middle, center
background-color: #f5f5f5

.pull-right[

## How can we use the training set to compare, evaluate, and tune models?

]

---

class: middle

&lt;img src="https://www.tidymodels.org/start/resampling/img/resampling.svg" width="80%" style="display: block; margin: auto;" /&gt;

---

## Cross-validation

&lt;img src="images/cross-validation/Slide2.png" width="80%" style="display: block; margin: auto;" /&gt;

---

## Cross-validation

&lt;img src="images/cross-validation/Slide3.png" width="80%" style="display: block; margin: auto;" /&gt;

---

## Cross-validation

&lt;img src="images/cross-validation/Slide4.png" width="80%" style="display: block; margin: auto;" /&gt;

---

## Cross-validation

&lt;img src="images/cross-validation/Slide5.png" width="80%" style="display: block; margin: auto;" /&gt;

---

## Cross-validation

&lt;img src="images/cross-validation/Slide6.png" width="80%" style="display: block; margin: auto;" /&gt;

---

## Cross-validation

&lt;img src="images/cross-validation/Slide7.png" width="80%" style="display: block; margin: auto;" /&gt;

---

## Cross-validation

&lt;img src="images/cross-validation/Slide8.png" width="80%" style="display: block; margin: auto;" /&gt;

---

## Cross-validation

&lt;img src="images/cross-validation/Slide9.png" width="80%" style="display: block; margin: auto;" /&gt;

---

## Cross-validation

&lt;img src="images/cross-validation/Slide10.png" width="80%" style="display: block; margin: auto;" /&gt;

---

## Cross-validation

&lt;img src="images/cross-validation/Slide11.png" width="80%" style="display: block; margin: auto;" /&gt;

---

## V-fold cross-validation


```r
vfold_cv(data, v = 10, ...)
```

---
exclude: true



---

## Guess

How many times does an observation/row appear in the assessment set?

&lt;img src="index_files/figure-html/vfold-tiles-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="index_files/figure-html/unnamed-chunk-105-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

## Quiz

If we use 10 folds, which percent of our data will end up in the training set and which percent in the testing set for each fold?

--

90% - training

10% - test

---

## Your Turn 4

Run the code below. What does it return?


```r
set.seed(100)
bechdel_folds &lt;- vfold_cv(data = bechdel_train, v = 10, strata = test)
bechdel_folds
```

<div class="countdown" id="timer_62f3e9e7" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">01</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---

```r
set.seed(100)
bechdel_folds &lt;- vfold_cv(data = bechdel_train, v = 10, strata = test)
bechdel_folds
## #  10-fold cross-validation using stratification 
## # A tibble: 10 Ã— 2
##    splits            id    
##    &lt;list&gt;            &lt;chr&gt; 
##  1 &lt;split [940/105]&gt; Fold01
##  2 &lt;split [940/105]&gt; Fold02
##  3 &lt;split [940/105]&gt; Fold03
##  4 &lt;split [940/105]&gt; Fold04
##  5 &lt;split [940/105]&gt; Fold05
##  6 &lt;split [940/105]&gt; Fold06
##  7 &lt;split [941/104]&gt; Fold07
##  8 &lt;split [941/104]&gt; Fold08
##  9 &lt;split [941/104]&gt; Fold09
## 10 &lt;split [942/103]&gt; Fold10
```

---

## We need a new way to fit


```r
split1       &lt;- bechdel_folds %&gt;% pluck("splits", 1)
split1_train &lt;- training(split1)
split1_test  &lt;- testing(split1)

tree_mod %&gt;% 
  fit(test ~ ., data = split1_train) %&gt;% 
  predict(split1_test) %&gt;% 
  mutate(truth = split1_test$test) %&gt;% 
  accuracy(truth, .pred_class)

# rinse and repeat
split2 &lt;- ...
```

---

## `fit_resamples()`

Trains and tests a resampled model.


```r
tree_mod %&gt;% 
  fit_resamples(
    test ~ metascore + imdb_rating, 
    resamples = bechdel_folds
    )
```

---


```r
tree_mod %&gt;% 
  fit_resamples(
    test ~ metascore + imdb_rating, 
    resamples = bechdel_folds
    )
## # Resampling results
## # 10-fold cross-validation using stratification 
## # A tibble: 10 Ã— 4
##    splits            id     .metrics         .notes  
##    &lt;list&gt;            &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;  
##  1 &lt;split [940/105]&gt; Fold01 &lt;tibble [2 Ã— 4]&gt; &lt;tibble&gt;
##  2 &lt;split [940/105]&gt; Fold02 &lt;tibble [2 Ã— 4]&gt; &lt;tibble&gt;
##  3 &lt;split [940/105]&gt; Fold03 &lt;tibble [2 Ã— 4]&gt; &lt;tibble&gt;
##  4 &lt;split [940/105]&gt; Fold04 &lt;tibble [2 Ã— 4]&gt; &lt;tibble&gt;
##  5 &lt;split [940/105]&gt; Fold05 &lt;tibble [2 Ã— 4]&gt; &lt;tibble&gt;
##  6 &lt;split [940/105]&gt; Fold06 &lt;tibble [2 Ã— 4]&gt; &lt;tibble&gt;
##  7 &lt;split [941/104]&gt; Fold07 &lt;tibble [2 Ã— 4]&gt; &lt;tibble&gt;
##  8 &lt;split [941/104]&gt; Fold08 &lt;tibble [2 Ã— 4]&gt; &lt;tibble&gt;
##  9 &lt;split [941/104]&gt; Fold09 &lt;tibble [2 Ã— 4]&gt; &lt;tibble&gt;
## 10 &lt;split [942/103]&gt; Fold10 &lt;tibble [2 Ã— 4]&gt; &lt;tibble&gt;
```

---

## `collect_metrics()`

Unnest the metrics column from a tidymodels `fit_resamples()`


```r
_results %&gt;% collect_metrics(summarize = TRUE)
```

--

.footnote[`TRUE` is actually the default; averages across folds]

---


```r
tree_mod %&gt;% 
  fit_resamples(
    test ~ metascore + imdb_rating, 
    resamples = bechdel_folds
    ) %&gt;% 
  collect_metrics(summarize = FALSE)
## # A tibble: 20 Ã— 5
##    id     .metric  .estimator .estimate .config             
##    &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
##  1 Fold01 accuracy binary         0.610 Preprocessor1_Model1
##  2 Fold01 roc_auc  binary         0.587 Preprocessor1_Model1
##  3 Fold02 accuracy binary         0.438 Preprocessor1_Model1
##  4 Fold02 roc_auc  binary         0.434 Preprocessor1_Model1
##  5 Fold03 accuracy binary         0.6   Preprocessor1_Model1
##  6 Fold03 roc_auc  binary         0.570 Preprocessor1_Model1
##  7 Fold04 accuracy binary         0.562 Preprocessor1_Model1
##  8 Fold04 roc_auc  binary         0.547 Preprocessor1_Model1
##  9 Fold05 accuracy binary         0.6   Preprocessor1_Model1
## 10 Fold05 roc_auc  binary         0.572 Preprocessor1_Model1
## # â€¦ with 10 more rows
## # â„¹ Use `print(n = ...)` to see more rows
```

---

## 10-fold CV

- 10 different analysis/assessment sets

- 10 different models (trained on .display[analysis] sets)

- 10 different sets of performance statistics (on .display[assessment] sets)

---

## Your Turn 5

Modify the code below to use `fit_resamples` and `bechdel_folds` to cross-validate the classification tree model. What is the ROC AUC that you collect at the end?


```r
set.seed(100)
tree_mod %&gt;% 
  fit(test ~ metascore + imdb_rating, 
      data = bechdel_train) %&gt;% 
  predict(new_data = bechdel_test) %&gt;% 
  mutate(true_bechdel = bechdel_test$test) %&gt;% 
  accuracy(truth = true_bechdel, estimate = .pred_class)
```

<div class="countdown" id="timer_62f3ebc2" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">03</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---


```r
set.seed(100)
tree_mod %&gt;% 
  fit_resamples(test ~ metascore + imdb_rating, 
                resamples = bechdel_folds) %&gt;% 
  collect_metrics()
## # A tibble: 2 Ã— 6
##   .metric  .estimator  mean     n std_err .config           
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;             
## 1 accuracy binary     0.560    10  0.0157 Preprocessor1_Modâ€¦
## 2 roc_auc  binary     0.547    10  0.0154 Preprocessor1_Modâ€¦
```

---

## How did we do?


```r
tree_mod %&gt;% 
  fit(test ~ metascore + imdb_rating, data = bechdel_train) %&gt;% 
  predict(bechdel_test) %&gt;% 
  mutate(truth = bechdel_test$test) %&gt;% 
  accuracy(truth, .pred_class)
## # A tibble: 1 Ã— 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.550
```


```
## # A tibble: 2 Ã— 6
##   .metric  .estimator  mean     n std_err .config           
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;             
## 1 accuracy binary     0.560    10  0.0157 Preprocessor1_Modâ€¦
## 2 roc_auc  binary     0.547    10  0.0154 Preprocessor1_Modâ€¦
```

    </textarea><style data-target=print-only>@media screen{.remark-slide-container{display:block}.remark-slide-scaler{box-shadow:none}}</style><script src=https://remarkjs.com/downloads/remark-latest.min.js></script><script>var slideshow=remark.create({highlightStyle:"magula",highlightLines:!0,highlightLanguage:"r",ratio:"16:9",countIncrementalSlides:!1});window.HTMLWidgets&&slideshow.on("afterShowSlide",function(){window.dispatchEvent(new Event("resize"))}),function(e){var t=e.createElement("style"),n=e.querySelector(".remark-slide-scaler");if(!n)return;t.type="text/css",t.innerHTML="@page {size: "+n.style.width+" "+n.style.height+"; }",e.head.appendChild(t)}(document),function(e){if(s=e.getElementsByClassName("remark-slides-area"),!s)return;for(var n,s,o,i=slideshow.getSlides(),a=s[0].children,t=1;t<i.length;t++)o=i[t],(o.properties.continued==="true"||o.properties.count==="false")&&(a[t-1].className+=" has-continuation");n=e.createElement("style"),n.type="text/css",n.innerHTML="@media print { .has-continuation { display: none; } }",e.head.appendChild(n)}(document),function(){var e=!1;slideshow.on("beforeShowSlide",function(){if(e)return;for(var n,o=document.styleSheets,s=0;s<o.length;s++){if(n=o[s].ownerNode,n.dataset.target!=="print-only")continue;n.parentNode.removeChild(n)}e=!0})}(),function(e){let t={};e.querySelectorAll(".remark-help-content table tr").forEach(e=>{const n=e.querySelector("td:nth-child(2)").innerText;e.querySelectorAll("td:first-child .key").forEach(e=>{const s=e.innerText;/^[a-z]$/.test(s)&&(t[s]=n)})}),e.body.setAttribute("data-at-shortcutkeys",JSON.stringify(t))}(document),function(){"use strict";var e,n,s,o,i,t=document.querySelectorAll(".remark-slides-area .remark-slide-container script");if(!t.length)return;for(e=0;e<t.length;e++){s=document.createElement("script"),i=document.createTextNode(t[e].textContent),s.appendChild(i);for(o=t[e].attributes,n=0;n<o.length;n++)s.setAttribute(o[n].name,o[n].value);t[e].parentElement.replaceChild(s,t[e])}}(),function(){for(var t=document.getElementsByTagName("a"),e=0;e<t.length;e++)/^(https?:)?\/\//.test(t[e].getAttribute("href"))&&(t[e].target="_blank")}(),function(e){const s=e.querySelectorAll(".remark-code-line-highlighted"),t=[],n=function(e,t=0){if(t>1)return null;const s=e.parentElement;return s.tagName==="PRE"?s:n(s,++t)};for(let o of s){let e=n(o);e&&!t.includes(e)&&t.push(e)}t.forEach(e=>e.classList.add("remark-code-has-line-highlighted"))}(document)</script><script>slideshow._releaseMath=function(e){var t,n,s,o=e.getElementsByTagName("code");for(s=0;s<o.length;){if(t=o[s],t.parentNode.tagName!=="PRE"&&t.childElementCount===0&&(n=t.textContent,/^\\\((.|\s)+\\\)$/.test(n)||/^\\\[(.|\s)+\\\]$/.test(n)||/^\$\$(.|\s)+\$\$$/.test(n)||/^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(n))){t.outerHTML=t.innerHTML;continue}s++}},slideshow._releaseMath(document)</script><script>(function(){var e=document.createElement("script");e.type="text/javascript",e.src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML",location.protocol!=="file:"&&/^https?:/.test(e.src)&&(e.src=e.src.replace(/^https?:/,"")),document.getElementsByTagName("head")[0].appendChild(e)})()</script></body></html></main><!doctype html><html><head><meta charset=utf-8><meta http-equiv=Content-Style-Type content="text/css"><title></title><meta name=Generator content="Cocoa HTML Writer"><meta name=CocoaVersion content="2299.7"><style type=text/css>p.p1{margin:0px;font:12px Times;-webkit-text-stroke:#000000}p.p2{margin:0px 0px 12px;font:12px Times;color:#0000e9;-webkit-text-stroke:#0000e9}li.li3{margin:0px;font:12px Times;-webkit-text-stroke:#000000;min-height:14px}span.s1{font-kerning:none}span.s2{font:10px Times;font-kerning:none}span.s3{text-decoration:underline;font-kerning:none}ul.ul1{list-style-type:disc}</style></head><body><p class=p1><span class=s1></span></p><p class=p1><span class=s1></span></p><p class=p1><span class=s1></span></p><p class=p1><span class=s1></span><span class=s2>Â© 2025 Jean Clipperton (materials adapted from Benjamin Soltoff and Sabrina Nardin) All Rights Reserved</span><span class=s1> <span class=Apple-converted-space>Â </span></span></p><p class=p1><span class=s1></span></p></body></html></div><script src=/js/main.min.0e22e4306907fbeadc1b0c467f61f4683be2ba222d4f5ea0ee4e90019a6f7200.js integrity="sha256-DiLkMGkH++rcGwxGf2H0aDviuiItT16g7k6QAZpvcgA=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>